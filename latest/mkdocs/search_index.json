{
    "docs": [
        {
            "location": "/", 
            "text": "Images.jl\n\n\nAn image processing library for \nJulia\n.\n\n\n\n\nInstallation\n\n\nInstall via the package manager,\n\n\nPkg.add(\nImages\n)\n\n\n\n\nIt's helpful to have ImageMagick installed on your system, as Images relies on it for reading and writing many common image types.  ImageMagick \nshould\n be installed for you automatically. In case of trouble, more details about manual installation and troubleshooting can be found in the \ninstallation help\n. Mac users in particular seem to have trouble; you may find \ndebugging Homebrew\n useful.\n\n\n\n\nPackage interactions\n\n\nA few other packages define overlapping functions or types (\nPyPlot\n defines \nimread\n, and \nWinston\n defines \nImage\n).  When using both Images and these packages, you can always specify which version you want with \nImages.imread(\"myimage.png\")\n.\n\n\n\n\nImage viewing\n\n\nIf you're using the \nIJulia\n notebook, images will be displayed \nautomatically\n.\n\n\nJulia code for the display of images can be found in \nImageView\n.  Installation of this package is recommended but not required.\n\n\n\n\nTestImages\n\n\nWhen testing ideas or just following along with the documentation, it can be useful to have some images to work with.  The \nTestImages\n package bundles several \"standard\" images for you.  To load one of the images from this package, say\n\n\nusing TestImages\nimg = testimage(\nmandrill\n)\n\n\n\n\nThe examples below will assume you're loading a particular file from your disk, but you can substitute those commands with \ntestimage\n.\n\n\n\n\nGetting started\n\n\nFor these examples you'll need to install both \nImages\n and \nImageView\n. Depending on your task, it's also very useful to have two other packages loaded, \nColors\n and \nFixedPointNumbers\n.  Load the code for all of these packages with\n\n\nusing Images, Colors, FixedPointNumbers, ImageView\n\n\n\n\n\n\nLoading your first image: how images are represented\n\n\nYou likely have a number of images already at your disposal, and you can use these, TestImages.jl, or run \nreadremote.jl\n in the \ntest/\n directory.  (This requires an internet connection.)  These will be deposited inside an \nImages\n directory inside your temporary directory (e.g., \n/tmp\n on Linux systems). The \n\"rose.png\"\n image in this example comes from the latter.\n\n\nLet's begin by reading an image from a file:\n\n\njulia\n img = imread(\nrose.png\n)\nRGB Image with:\n  data: 70x46 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1\n\n\n\n\nIf you're using Images through IJulia, rather than this text output you probably see the image itself.  This is nice, but often it's quite helpful to see the structure of these Image objects.  This happens automatically at the REPL; within IJulia you can call\n\n\nshow(img)\n\n\n\n\nto see the output above.\n\n\nThe first line tells you that this is an RGB image. It is stored as a two-dimensional \nArray\n of \nRGB{UFixed{Uint8,8}}\n. To see what this pixel type is, we can do the following:\n\n\njulia\n img[1,1]\nRGB{UFixed8}(0.188,0.184,0.176)\n\n\n\n\nThis extracts the first pixel, the one visually at the upper-left of the image. You can see that an \nRGB\n (which comes from the \nColors\n package) is a triple of values. The \nUFixed8\n number type (which comes from the \nFixedPointNumbers\n package), and whose long name is \nUFixed{Uint8,8}\n) represents fractional numbers, those that can encode values that lie between 0 and 1, using just 1 byte (8 bits).  If you've previously used other image processing libraries, you may be used to thinking of two basic image types, floating point-valued and integer-valued. In those libraries, \"saturated\" (the color white for an RGB image) would be represented by \n1.0\n for floating point-valued images, 255 for a \nUint8\n image, and \n0x0fff\n for an image collected by a 12-bit camera. \nImages.jl\n, via Colors and FixedPointNumbers, unifies these so that \n1\n always means saturated, no matter whether the element type is \nFloat64\n, \nUFixed8\n, or \nUFixed12\n.  This makes it easier to write generic algorithms and visualization code, while still allowing one to use efficient (and C-compatible) raw representations.\n\n\nYou can see that this image has \nproperties\n, of which there are three: \n\"IMcs\"\n, \n\"spatialorder\"\n and \n\"pixelspacing\"\n.  We'll talk more about the latter two in the next section.  The \n\"IMcs\"\n is really for internal use by ImageMagick; it says that the colorspace is \n\"sRGB\"\n, although (depending on which version of the library you have) you may see it say \n\"RGB\"\n.  Such differences are due to \nchanges\n in how ImageMagick handles colorspaces, and the fact that both older and newer versions of the library are still widespread.\n\n\nYou can retrieve the properties using \nprops = properties(img)\n. This returns the dictionary used by \nimg\n; any modifications you make to \nprops\n will update the properties of \nimg\n.\n\n\nLikewise, given an Image \nimg\n, you can access the underlying array with\n\n\nA = data(img)\n\n\n\n\nThis is handy for those times when you want to call an algorithm that is implemented only for \nArray\ns. At the end, however, you may want to restore the contextual information available in an Image. While you can use the \nImage\n constructor directly, two alternatives can be convenient:\n\n\nimgc = copyproperties(img, A)\nimgs = shareproperties(img, A)\n\n\n\n\nimgc\n has its own properties dictionary, initialized to be a copy of the one used by \nimg\n.  In contrast, \nimgs\n shares a properties dictionary with \nimg\n; any modification to the properties of \nimg\n will also modify them for \nimgs\n. Use either as appropriate to your circumstance.\n\n\nThe Images package is designed to work with either plain arrays or with Image types\u2013-in general, though, you're probably best off leaving things as an Image, particularly if you work with movies, 3d images, or other more complex objects.\n\n\n\n\nStorage order and changing the representation of images\n\n\nIn the example above, the \n\"spatialorder\"\n property has value \n[\"x\", \"y\"]\n. This indicates that the image data are in \"horizontal-major\" order, meaning that a pixel at spatial location \n(x,y)\n would be addressed as \nimg[x,y]\n rather than \nimg[y,x]\n. \n[\"y\", \"x\"]\n would indicate vertical-major.  Consequently, this image is 70 pixels wide and 46 pixels high.\n\n\nImages returns this image in horizontal-major order because this is how it was stored on disk.  Because the Images package is designed to scale to terabyte-sized images, a general philosophy is to work with whatever format users provide without forcing changes to the raw array representation. Consequently, when you load an image, its representation will match that used in the file.\n\n\nOf course, if you prefer to work with plain arrays, you can convert it:\n\n\njulia\n imA = convert(Array, img);\n\njulia\n summary(imA)\n\n46x70 Array{RGB{UFixed{Uint8,8}},2}\n\n\n\n\n\nYou can see that this permuted the dimensions into vertical-major order, consistent with the column-major order with which Julia stores \nArrays\n. Note that this preserved the element type, returning an \nArray{RGB}\n.  If you prefer to extract into an array of plain numbers in color-last order (typical of Matlab), you can use\n\n\njulia\n imsep = separate(img)\nRGB Image with:\n  data: 46x70x3 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 3\n    spatialorder:  y x\n    pixelspacing:  1 1\n\n\n\n\nYou can see that \n\"spatialorder\"\n was changed to reflect the new layout, and that two new properties were added: \n\"colordim\"\n, which specifies which dimension of the array is used to encode color, and \n\"colorspace\"\n so you know how to interpret these colors.\n\n\nCompare this to\n\n\njulia\n imr = reinterpret(UFixed8, img)\nRGB Image with:\n  data: 3x70x46 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 1\n    spatialorder:  x y\n    pixelspacing:  1 1\n\n\n\n\nreinterpret\n gives you a new view of the same underlying memory as \nimg\n, whereas \nconvert(Array, img)\n and \nseparate(img)\n create new arrays if the memory-layout needs alteration.\n\n\nYou can go back to using Colors to encode your image this way:\n\n\njulia\n imcomb = convert(Image{RGB}, imsep)\nRGB Image with:\n  data: 46x70 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  y x\n    pixelspacing:  1 1\n\n\n\n\nor even change to a new colorspace like this:\n\n\njulia\n imhsv = convert(Image{HSV}, float32(img))\nHSV Image with:\n  data: 70x46 Array{HSV{Float32},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1\n\n\n\n\nMany of the colorspaces supported by Colors (or rather its base package, ColorTypes) need a wider range of values than \n[0,1]\n, so it's necessary to convert to floating point.\n\n\nIf you say \nview(imhsv)\n, you may be surprised to see something that looks like the original RGB image. Since the colorspace is known, it converts to RGB before rendering it. If, for example, you wanted to see what a \"pure-V\" image looks like, you can do this:\n\n\nimv = shareproperties(imhsv, [HSV(0, 0, imhsv[i,j].v) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imv)\n\n\n\n\nand a pure-H image like this:\n\n\nimh = shareproperties(imhsv, [HSV(imhsv[i,j].h, 0.5, 0.5) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imh)\n\n\n\n\n(Hue without saturation or value generates gray or black, so we used a constant different from zero for these parameters.)\n\n\n \n\n\n\nOf course, you can combine these commands, for example\n\n\nA = reinterpret(Uint8, data(img))\n\n\n\n\nwill, for a \nRGB{UFixed8}\n image, return a raw 3d array.  This can be useful if you want to interact with external code (a C-library, for example).  Assuming you don't want to lose orientation information, you can wrap a returned array \nB\n as \nshareproperties(img, B)\n.\n\n\n\n\nOther properties, and usage of Units\n\n\nThe \n\"pixelspacing\"\n property informs ImageView that this image has an aspect ratio 1.  In scientific or medical imaging, you can use actual units to encode this property, for example through the \nSIUnits\n package.  For example, if you're doing microscopy you might specify\n\n\nusing SIUnits\nimg[\npixelspacing\n] = [0.32Micro*Meter,0.32Micro*Meter]\n\n\n\n\nIf you're performing three-dimensional imaging, you might set different values for the different axes:\n\n\nusing SIUnits.ShortUnits\nmriscan[\npixelspacing\n] = [0.2mm, 0.2mm, 2mm]\n\n\n\n\nImageView includes facilities for scale bars, and by supplying your pixel spacing you can ensure that the scale bars are accurate.\n\n\n\n\nA brief demonstration of image processing\n\n\nNow let's work through a more sophisticated example:\n\n\nusing Images, TestImages, ImageView\nimg = testimage(\nmandrill\n)\nview(img)\n# Let's do some blurring\nkern = ones(Float32,7,7)/49\nimgf = imfilter(img, kern)\nview(imgf)\n# Let's make an oversaturated image\nimgs = 2imgf\nview(imgs)\n\n\n\n\n \n\n\n\n\n\nFurther documentation\n\n\nDetailed documentation about the design of the library and the available functions can be found in the navigation list to the right. Here are some of the topics available:\n\n\n\n\nThe \ncore\n representation of images\n\n\nFunction reference\n\n\nOverlays\n, a type for combining multiple grayscale arrays   into a single color array\n\n\n\n\n\n\nCredits\n\n\nElements of this package descend from \"image.jl\" that once lived in Julia's \nextras/\n directory.  That file had several authors, of which the primary were Jeff Bezanson, Stefan Kroboth, Tim Holy, Mike Nolta, and Stefan Karpinski.  This repository has been quite heavily reworked; the current package maintainer is Tim Holy, and important contributions have been made by Ron Rock, Kevin Squire, Lucas Beyer, Elliot Saba, Isaiah Norton, Daniel Perry, Waldir Pimenta, Tobias Knopp, Jason Merrill, Dahua Lin, and several others.", 
            "title": "Home"
        }, 
        {
            "location": "/#imagesjl", 
            "text": "An image processing library for  Julia .", 
            "title": "Images.jl"
        }, 
        {
            "location": "/#installation", 
            "text": "Install via the package manager,  Pkg.add( Images )  It's helpful to have ImageMagick installed on your system, as Images relies on it for reading and writing many common image types.  ImageMagick  should  be installed for you automatically. In case of trouble, more details about manual installation and troubleshooting can be found in the  installation help . Mac users in particular seem to have trouble; you may find  debugging Homebrew  useful.", 
            "title": "Installation"
        }, 
        {
            "location": "/#package-interactions", 
            "text": "A few other packages define overlapping functions or types ( PyPlot  defines  imread , and  Winston  defines  Image ).  When using both Images and these packages, you can always specify which version you want with  Images.imread(\"myimage.png\") .", 
            "title": "Package interactions"
        }, 
        {
            "location": "/#image-viewing", 
            "text": "If you're using the  IJulia  notebook, images will be displayed  automatically .  Julia code for the display of images can be found in  ImageView .  Installation of this package is recommended but not required.", 
            "title": "Image viewing"
        }, 
        {
            "location": "/#testimages", 
            "text": "When testing ideas or just following along with the documentation, it can be useful to have some images to work with.  The  TestImages  package bundles several \"standard\" images for you.  To load one of the images from this package, say  using TestImages\nimg = testimage( mandrill )  The examples below will assume you're loading a particular file from your disk, but you can substitute those commands with  testimage .", 
            "title": "TestImages"
        }, 
        {
            "location": "/#getting-started", 
            "text": "For these examples you'll need to install both  Images  and  ImageView . Depending on your task, it's also very useful to have two other packages loaded,  Colors  and  FixedPointNumbers .  Load the code for all of these packages with  using Images, Colors, FixedPointNumbers, ImageView", 
            "title": "Getting started"
        }, 
        {
            "location": "/#loading-your-first-image-how-images-are-represented", 
            "text": "You likely have a number of images already at your disposal, and you can use these, TestImages.jl, or run  readremote.jl  in the  test/  directory.  (This requires an internet connection.)  These will be deposited inside an  Images  directory inside your temporary directory (e.g.,  /tmp  on Linux systems). The  \"rose.png\"  image in this example comes from the latter.  Let's begin by reading an image from a file:  julia  img = imread( rose.png )\nRGB Image with:\n  data: 70x46 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1  If you're using Images through IJulia, rather than this text output you probably see the image itself.  This is nice, but often it's quite helpful to see the structure of these Image objects.  This happens automatically at the REPL; within IJulia you can call  show(img)  to see the output above.  The first line tells you that this is an RGB image. It is stored as a two-dimensional  Array  of  RGB{UFixed{Uint8,8}} . To see what this pixel type is, we can do the following:  julia  img[1,1]\nRGB{UFixed8}(0.188,0.184,0.176)  This extracts the first pixel, the one visually at the upper-left of the image. You can see that an  RGB  (which comes from the  Colors  package) is a triple of values. The  UFixed8  number type (which comes from the  FixedPointNumbers  package), and whose long name is  UFixed{Uint8,8} ) represents fractional numbers, those that can encode values that lie between 0 and 1, using just 1 byte (8 bits).  If you've previously used other image processing libraries, you may be used to thinking of two basic image types, floating point-valued and integer-valued. In those libraries, \"saturated\" (the color white for an RGB image) would be represented by  1.0  for floating point-valued images, 255 for a  Uint8  image, and  0x0fff  for an image collected by a 12-bit camera.  Images.jl , via Colors and FixedPointNumbers, unifies these so that  1  always means saturated, no matter whether the element type is  Float64 ,  UFixed8 , or  UFixed12 .  This makes it easier to write generic algorithms and visualization code, while still allowing one to use efficient (and C-compatible) raw representations.  You can see that this image has  properties , of which there are three:  \"IMcs\" ,  \"spatialorder\"  and  \"pixelspacing\" .  We'll talk more about the latter two in the next section.  The  \"IMcs\"  is really for internal use by ImageMagick; it says that the colorspace is  \"sRGB\" , although (depending on which version of the library you have) you may see it say  \"RGB\" .  Such differences are due to  changes  in how ImageMagick handles colorspaces, and the fact that both older and newer versions of the library are still widespread.  You can retrieve the properties using  props = properties(img) . This returns the dictionary used by  img ; any modifications you make to  props  will update the properties of  img .  Likewise, given an Image  img , you can access the underlying array with  A = data(img)  This is handy for those times when you want to call an algorithm that is implemented only for  Array s. At the end, however, you may want to restore the contextual information available in an Image. While you can use the  Image  constructor directly, two alternatives can be convenient:  imgc = copyproperties(img, A)\nimgs = shareproperties(img, A)  imgc  has its own properties dictionary, initialized to be a copy of the one used by  img .  In contrast,  imgs  shares a properties dictionary with  img ; any modification to the properties of  img  will also modify them for  imgs . Use either as appropriate to your circumstance.  The Images package is designed to work with either plain arrays or with Image types\u2013-in general, though, you're probably best off leaving things as an Image, particularly if you work with movies, 3d images, or other more complex objects.", 
            "title": "Loading your first image: how images are represented"
        }, 
        {
            "location": "/#storage-order-and-changing-the-representation-of-images", 
            "text": "In the example above, the  \"spatialorder\"  property has value  [\"x\", \"y\"] . This indicates that the image data are in \"horizontal-major\" order, meaning that a pixel at spatial location  (x,y)  would be addressed as  img[x,y]  rather than  img[y,x] .  [\"y\", \"x\"]  would indicate vertical-major.  Consequently, this image is 70 pixels wide and 46 pixels high.  Images returns this image in horizontal-major order because this is how it was stored on disk.  Because the Images package is designed to scale to terabyte-sized images, a general philosophy is to work with whatever format users provide without forcing changes to the raw array representation. Consequently, when you load an image, its representation will match that used in the file.  Of course, if you prefer to work with plain arrays, you can convert it:  julia  imA = convert(Array, img);\n\njulia  summary(imA) 46x70 Array{RGB{UFixed{Uint8,8}},2}   You can see that this permuted the dimensions into vertical-major order, consistent with the column-major order with which Julia stores  Arrays . Note that this preserved the element type, returning an  Array{RGB} .  If you prefer to extract into an array of plain numbers in color-last order (typical of Matlab), you can use  julia  imsep = separate(img)\nRGB Image with:\n  data: 46x70x3 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 3\n    spatialorder:  y x\n    pixelspacing:  1 1  You can see that  \"spatialorder\"  was changed to reflect the new layout, and that two new properties were added:  \"colordim\" , which specifies which dimension of the array is used to encode color, and  \"colorspace\"  so you know how to interpret these colors.  Compare this to  julia  imr = reinterpret(UFixed8, img)\nRGB Image with:\n  data: 3x70x46 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 1\n    spatialorder:  x y\n    pixelspacing:  1 1  reinterpret  gives you a new view of the same underlying memory as  img , whereas  convert(Array, img)  and  separate(img)  create new arrays if the memory-layout needs alteration.  You can go back to using Colors to encode your image this way:  julia  imcomb = convert(Image{RGB}, imsep)\nRGB Image with:\n  data: 46x70 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  y x\n    pixelspacing:  1 1  or even change to a new colorspace like this:  julia  imhsv = convert(Image{HSV}, float32(img))\nHSV Image with:\n  data: 70x46 Array{HSV{Float32},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1  Many of the colorspaces supported by Colors (or rather its base package, ColorTypes) need a wider range of values than  [0,1] , so it's necessary to convert to floating point.  If you say  view(imhsv) , you may be surprised to see something that looks like the original RGB image. Since the colorspace is known, it converts to RGB before rendering it. If, for example, you wanted to see what a \"pure-V\" image looks like, you can do this:  imv = shareproperties(imhsv, [HSV(0, 0, imhsv[i,j].v) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imv)  and a pure-H image like this:  imh = shareproperties(imhsv, [HSV(imhsv[i,j].h, 0.5, 0.5) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imh)  (Hue without saturation or value generates gray or black, so we used a constant different from zero for these parameters.)     Of course, you can combine these commands, for example  A = reinterpret(Uint8, data(img))  will, for a  RGB{UFixed8}  image, return a raw 3d array.  This can be useful if you want to interact with external code (a C-library, for example).  Assuming you don't want to lose orientation information, you can wrap a returned array  B  as  shareproperties(img, B) .", 
            "title": "Storage order and changing the representation of images"
        }, 
        {
            "location": "/#other-properties-and-usage-of-units", 
            "text": "The  \"pixelspacing\"  property informs ImageView that this image has an aspect ratio 1.  In scientific or medical imaging, you can use actual units to encode this property, for example through the  SIUnits  package.  For example, if you're doing microscopy you might specify  using SIUnits\nimg[ pixelspacing ] = [0.32Micro*Meter,0.32Micro*Meter]  If you're performing three-dimensional imaging, you might set different values for the different axes:  using SIUnits.ShortUnits\nmriscan[ pixelspacing ] = [0.2mm, 0.2mm, 2mm]  ImageView includes facilities for scale bars, and by supplying your pixel spacing you can ensure that the scale bars are accurate.", 
            "title": "Other properties, and usage of Units"
        }, 
        {
            "location": "/#a-brief-demonstration-of-image-processing", 
            "text": "Now let's work through a more sophisticated example:  using Images, TestImages, ImageView\nimg = testimage( mandrill )\nview(img)\n# Let's do some blurring\nkern = ones(Float32,7,7)/49\nimgf = imfilter(img, kern)\nview(imgf)\n# Let's make an oversaturated image\nimgs = 2imgf\nview(imgs)", 
            "title": "A brief demonstration of image processing"
        }, 
        {
            "location": "/#further-documentation", 
            "text": "Detailed documentation about the design of the library and the available functions can be found in the navigation list to the right. Here are some of the topics available:   The  core  representation of images  Function reference  Overlays , a type for combining multiple grayscale arrays   into a single color array", 
            "title": "Further documentation"
        }, 
        {
            "location": "/#credits", 
            "text": "Elements of this package descend from \"image.jl\" that once lived in Julia's  extras/  directory.  That file had several authors, of which the primary were Jeff Bezanson, Stefan Kroboth, Tim Holy, Mike Nolta, and Stefan Karpinski.  This repository has been quite heavily reworked; the current package maintainer is Tim Holy, and important contributions have been made by Ron Rock, Kevin Squire, Lucas Beyer, Elliot Saba, Isaiah Norton, Daniel Perry, Waldir Pimenta, Tobias Knopp, Jason Merrill, Dahua Lin, and several others.", 
            "title": "Credits"
        }, 
        {
            "location": "/aims/", 
            "text": "Aims\n\n\nImages are very diverse.  You might be working with a single photograph, or you might be processing MRI scans from databases of hundreds of subjects.  In the former case, you might not need much information about the image; perhaps just the pixel data itself suffices.  In the latter case, you probably need to know a lot of extra details, like the patient's ID number and characteristics of the image like the physical size of a voxel in all three dimensions.\n\n\nEven the raw pixel data can come in several different flavors. For example, you might represent each pixel as a \nUint32\n because you are encoding red, green, and blue in separate 8-bit words within each integer. Visualization libraries like Cairo use these kinds of representations, and you might want to interact with those libraries efficiently.  Alternatively, perhaps you're an astronomer and your camera has such high precision that 16 bits aren't enough to encode grayscale intensities. If you're working with videos (images collected over time), you might have arrays that are too big to load into memory at once.  You still need to be able to \"talk about\" the array as a whole, but it may not be trivial to adjust the byte-level representation to match some pre-conceived storage order.\n\n\nTo handle this diversity, we've endeavored to take a \"big tent\" philosophy.  We avoid imposing a strict programming model, because we don't want to make life difficult for people who have relatively simple needs.  If you do all your image processing with plain arrays (as is typical in Matlab, for example), that should work just fine. You just have to respect certain conventions, like a \nm\n-by-\nn\n-by-\n3\n array always means an RGB image with the third dimension encoding color.  You can call the routines that are in this package, and write your own custom algorithms that assume the same format.\n\n\nBut if your images don't fit neatly into these assumptions, you can choose to represent your images using other schemes; you can then tag them with enough metadata that there's no ambiguity about the meaning of anything.  The algorithms in this package are already set to look for certain types of metadata, and adjust their behavior accordingly.\n\n\nOne of the potential downsides of flexibility is complexity; it makes it harder to write generic algorithms that work with all these different representations. We've tried to mitigate this downside by providing many short utility functions that abstract away much of the complexity.  Many algorithms require just a handful of extra lines to work generically.  Or if you just want to get something running, it usually only takes a couple of lines of code to assert that the input is in the format you expect.", 
            "title": "Aims"
        }, 
        {
            "location": "/aims/#aims", 
            "text": "Images are very diverse.  You might be working with a single photograph, or you might be processing MRI scans from databases of hundreds of subjects.  In the former case, you might not need much information about the image; perhaps just the pixel data itself suffices.  In the latter case, you probably need to know a lot of extra details, like the patient's ID number and characteristics of the image like the physical size of a voxel in all three dimensions.  Even the raw pixel data can come in several different flavors. For example, you might represent each pixel as a  Uint32  because you are encoding red, green, and blue in separate 8-bit words within each integer. Visualization libraries like Cairo use these kinds of representations, and you might want to interact with those libraries efficiently.  Alternatively, perhaps you're an astronomer and your camera has such high precision that 16 bits aren't enough to encode grayscale intensities. If you're working with videos (images collected over time), you might have arrays that are too big to load into memory at once.  You still need to be able to \"talk about\" the array as a whole, but it may not be trivial to adjust the byte-level representation to match some pre-conceived storage order.  To handle this diversity, we've endeavored to take a \"big tent\" philosophy.  We avoid imposing a strict programming model, because we don't want to make life difficult for people who have relatively simple needs.  If you do all your image processing with plain arrays (as is typical in Matlab, for example), that should work just fine. You just have to respect certain conventions, like a  m -by- n -by- 3  array always means an RGB image with the third dimension encoding color.  You can call the routines that are in this package, and write your own custom algorithms that assume the same format.  But if your images don't fit neatly into these assumptions, you can choose to represent your images using other schemes; you can then tag them with enough metadata that there's no ambiguity about the meaning of anything.  The algorithms in this package are already set to look for certain types of metadata, and adjust their behavior accordingly.  One of the potential downsides of flexibility is complexity; it makes it harder to write generic algorithms that work with all these different representations. We've tried to mitigate this downside by providing many short utility functions that abstract away much of the complexity.  Many algorithms require just a handful of extra lines to work generically.  Or if you just want to get something running, it usually only takes a couple of lines of code to assert that the input is in the format you expect.", 
            "title": "Aims"
        }, 
        {
            "location": "/core/", 
            "text": "Julia Images Guide\n\n\n\n\nThe basic types\n\n\n\n\nPlain arrays\n\n\nImages can be plain arrays, which are interpreted to be in \"Matlab format\": the first two dimensions are height (\nh\n) and width (\nw\n), a storage order here called \"vertical-major\". This ordering is inspired by the column/row index order of matrices and the desire to have a displayed image look like what one sees when a matrix is written out in text.\n\n\nIf you're working with RGB color, your best approach is to encode color as a \nColor\n, as defined in the \nColor\n package.  That package provides many utility functions for analyzing and manipulating colors.  Alternatively, you can use a third dimension of size 3, or encode your images as either \nRGB24\n or \nARGB32\n, which use an internal \nUint32\n representation of color.\n\n\nIt's worth noting that these Matlab conventions are sometimes inconvenient.  For example, the \nx\n coordinate (horizontal) is second and the \ny\n coordinate (vertical) is first; in other words, one uses \nimg[y,x]\n to address a pixel that is displayed at a particular \nx,y\n position. This often catches newcomers (and sometimes even old-timers) by surprise.  Moreover, most image file formats, cameras, and graphics libraries such as Cairo use \"horizontal-major\" storage of images, and have the color dimension first (fastest). The native Image type\u2013-which allows arbitrary ordering of the data array\u2013-permits you to use this raw representation directly, but when using plain arrays you need to permute the dimensions of the raw data array.\n\n\nThe convention that a \nm x n x 3\n array implies RGB is also problematic for anyone doing 3d imaging, and can result in hard-to-find bugs when the third dimension happens to be of size 3. For 3d imaging, the use of an Image type\u2013-perhaps converting Arrays via \ngrayim\n\u2013-is highly recommended.\n\n\nThe conventions for plain arrays are \"baked in\" via a few simple utility functions in the file \ncore.jl\n; if you really need to use plain arrays but want to work with different conventions, you can (locally) change these defaults with just a few lines. Algorithms which have been written generically should continue to work.\n\n\nHowever, a more flexible approach is to use one of the self-documenting image types.\n\n\n\n\nImage types\n\n\nAll image types should descend from \nAbstractImage\n, an abstract base type used to indicate that an array is to be interpreted as an image. If you're writing a custom image type, it is more likely that you'll want to derive from either \nAbstractImageDirect\n or \nAbstractImageIndexed\n. The former is for direct images (where intensity at a pixel is represented directly), the latter for indexed images (where intensity is looked up in a colormap table).\n\n\nIn practice, it is assumed that \nAbstractImages\n have at least two fields, called \ndata\n and \nproperties\n. (In code, you should not use these directly, instead using the functions \ndata\n and \nproperties\n to extract these.)  These are the only two fields in the first concrete image type, called \nImage\n:\n\n\ntype Image{T,N,A\n:AbstractArray} \n: AbstractImageDirect{T,N}\n    data::A\n    properties::Dict{String,Any}\nend\n\n\n\n\ndata\n stores the actual image data, and is an \nAbstractArray\n. This fact alone is the basis for a great deal of customizability: \ndata\n might be a plain \nArray\n stored in memory, a \nSubArray\n, a memory-mapped array (which is still just an \nArray\n), a custom type that stores additional information about \"missing data\" (like bad pixels or dropped frames), or a custom type that seamlessly presents views of a large number of separate files.  One concrete example in the Images codebase is the color \nOverlay\n \ntype\n.  If you have a suitably-defined \nAbstractArray\n type, you can probably use \nImage\n without needing to create alternative \nAbstractImageDirect\n types.\n\n\nproperties\n is a dictionary, with \nString\n keys, that allows you to annotate images. More detail about this point can be found below.\n\n\nThe only other concrete image type is for indexed images:\n\n\ntype ImageCmap{T,N,A\n:AbstractArray,C\n:AbstractArray} \n: AbstractImageIndexed{T,N}\n    data::A\n    cmap::C\n    properties::Dict{String,Any}\nend\n\n\n\n\nThe \ndata\n array here just encodes the index used to look up the color in the \ncmap\n field.\n\n\n\n\nAddressing image data\n\n\nFor any valid image type, \ndata(img)\n returns the array that corresponds to the image.  This works when \nimg\n is a plain \nArray\n (in which case no operation is performed) as well as for an \nImage\n (in which case it returns \nimg.data\n). For some image formats, Images.jl may interpret raw data with the \nFixedPointNumbers\n package. The function \nraw(img)\n can be used to recover the buffer in its raw format (e.g. \nUInt8\n). This is our first example of how to write generic algorithms.\n\n\nIf \nimg\n is an \nImage\n, then \nimg[i,j]\n looks up the value \nimg.data[i,j]\n. Assignment, \nsub\n, and \nslice\n work similarly. In other words, for indexing an \nImage\n works just as if you were using plain arrays.\n\n\nIf you load your image data using Image's \nimread\n, note that the storage order is not changed from the on-disk representation. Therefore, a 2D RGB image will most likely be stored in color-horizontal-vertical order, meaning that a pixel at \n(x,y)\n is accessed as \nimg[x,y]\n. Note that this is quite different from Matlab's default representation.\n\n\nIf you are indexing over an extended region and want to get back an \nImage\n, rather than a value or an \nArray\n, then you will want to use \ngetindexim\n, \nsubim\n, and \nsliceim\n. For the first two, the resulting image will share everything but the \ndata\n field with the original image; if you make modifications in one, the other will also be affected. For \nsliceim\n, because it can change the dimensionality some adjustments to \nproperties\n are needed; in this case a copy is made.\n\n\nOne of the properties (see below) that you can grant to images is \nspatialorder\n, which provides a name for each spatial dimension in the image. Using this feature, you can cut out regions or slices from images in the following ways:\n\n\nA = img[\nx\n, 200:400, \ny\n, 500:700]\nimgs = sliceim(img, \nz\n, 14)      # cuts out the 14th frame in a stack\n\n\n\n\nThese routines \"do the right thing\" no matter what storage order is being used.\n\n\n\n\nImage properties and accessor functions\n\n\nThe \nproperties\n dictionary can contain any information you want to store along with your images. Typically, each property is also affiliated with an accessor function of the same name.\n\n\nLet's illustrate this with one of the default properties, \n\"colorspace\"\n. The value of this property is a string, such as \n\"RGB\"\n or \n\"Gray\"\n or \n\"HSV\"\n. You can extract the value of this field using a function:\n\n\ncs = colorspace(img)\n\n\n\n\nThe reason to have a function, rather than just looking it up in the \nproperties\n dictionary, is that we can provide defaults. For example, images represented as plain \nArray\ns don't have a \nproperties\n dictionary; if we are to write generic code, we don't want to have to wonder whether this information is available. So for plain arrays, there are a number of defaults specified for the output of the \ncolorspace\n function, depending on the element type and size of the array. Likewise, images stored as \nColor\n arrays have no need of a \n\"colorspace\"\n property, because the colorspace is encoded in the type parameters.\n\n\nHere is a list of the properties supported in \ncore.jl\n:\n\n\n\n\ncolorspace\n: \"RGB\", \"RGBA\", \"Gray\", \"Binary\", \"24bit\", \"Lab\", \"HSV\", etc.  If   your image is represented as a Color array, you cannot override that   choice by specifying a \ncolorspace\n property.  (Use \nreinterpret\n if you want   to change the interpretation without changing the raw values.)\n\n\ncolordim\n: the array dimension used to store color information, or 0 if there   is no dimension corresponding to color\n\n\ntimedim\n: the array dimension used for time (i.e., sequence), or 0 for single   images\n\n\nscalei\n: a property that controls default contrast scaling upon display.   This should be a   \nMapInfo\n   value, to be used for setting the contrast upon display. In the absence of   this property, the range 0 to 1 will be used.\n\n\npixelspacing\n: the spacing between adjacent pixels along spatial dimensions\n\n\nspacedirections\n: more detailed information about the orientation of array   axes relative to an external coordinate system (see the   \nfunction reference\n).\n\n\nspatialorder\n: a string naming each spatial dimension of the array, in the   storage order of the data array.  Names can be arbitrary, but the choices \"x\"   and \"y\" have special meaning (horizontal and vertical, respectively,   irrespective of storage order).  If supplied, you must have one entry per   spatial dimension.\n\n\n\n\nIf you specify their values in the \nproperties\n dictionary, your values will be used; if not, hopefully-reasonable defaults will be chosen.\n\n\nNaturally, you can add whatever additional properties you want: you could add the date/time at which the image was captured, the patient ID, etc. The main point of having a properties dictionary, rather than a type with fixed fields, is the flexibility of adding whatever metadata you find to be useful.\n\n\n\n\nWriting generic algorithms\n\n\nLet's say you have an algorithm implemented for \nArray\ns, and you want to extend it to work on \nImage\n types. Let's consider the example of a hypothetical \nimfilter\n, written to perform kernel-based filtering in arbitrary dimensions. Let's say your \nimfilter\n looks like this:\n\n\nfunction imfilter{T,N}(A::Array{T,N}, kernel::Array{T,N}, options...)\n\n\n\n\nThe first step might be to simply provide a version for \nAbstractImage\n types:\n\n\nfunction imfilter{T,N}(img::AbstractImage{T,N}, kernel::Array{T,N}, options...)\n    out = imfilter(data(img), kernel, options...)\n    shareproperties(img, out)\nend\n\n\n\n\nNow let's say you additionally want to allow the user to filter color images\u2013-where one dimension of the array is used to encode color\u2013-with a filter of dimension \nN-1\n applied to each color channel separately. We can implement this version simultaneously for both \nImage\n types and other array types as follows:\n\n\nfunction imfilter{T,N,N1}(img::AbstractArray{T,N}, kernel::Array{T,N1}, options...)\n    cd = colordim(img)\n    if N1 != N - (cd != 0)\n        error(\nkernel has the wrong dimensionality\n)\n    end\n    out = similar(img)\n    for i = size(img, cd)\n        imsl = img[\ncolor\n, i]\n        outsl = slice(out, \ncolor\n, i)\n        copy!(outsl, imfilter(imsl, kernel, options...))\n    end\n    out\nend\n\n\n\n\nThere are other ways to achieve a similar effect; if you examine the actual implementation of \nimfilter\n, you'll see that the kernel is reshaped to be commensurate with the data array.\n\n\nThese solutions work no matter which dimension is used to store color, a feat that would be essentially impossible to achieve robustly in a generic algorithm if we didn't exploit metadata. Note also that if the user supplies an \nArray\n, s/he will get an \nArray\n back, and if using an \nImage\n will get an \nImage\n back with properties inherited from \nimg\n.\n\n\nNaturally, you can find other examples of generic implementations throughout the source code of \nImages\n.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/core/#julia-images-guide", 
            "text": "", 
            "title": "Julia Images Guide"
        }, 
        {
            "location": "/core/#the-basic-types", 
            "text": "", 
            "title": "The basic types"
        }, 
        {
            "location": "/core/#plain-arrays", 
            "text": "Images can be plain arrays, which are interpreted to be in \"Matlab format\": the first two dimensions are height ( h ) and width ( w ), a storage order here called \"vertical-major\". This ordering is inspired by the column/row index order of matrices and the desire to have a displayed image look like what one sees when a matrix is written out in text.  If you're working with RGB color, your best approach is to encode color as a  Color , as defined in the  Color  package.  That package provides many utility functions for analyzing and manipulating colors.  Alternatively, you can use a third dimension of size 3, or encode your images as either  RGB24  or  ARGB32 , which use an internal  Uint32  representation of color.  It's worth noting that these Matlab conventions are sometimes inconvenient.  For example, the  x  coordinate (horizontal) is second and the  y  coordinate (vertical) is first; in other words, one uses  img[y,x]  to address a pixel that is displayed at a particular  x,y  position. This often catches newcomers (and sometimes even old-timers) by surprise.  Moreover, most image file formats, cameras, and graphics libraries such as Cairo use \"horizontal-major\" storage of images, and have the color dimension first (fastest). The native Image type\u2013-which allows arbitrary ordering of the data array\u2013-permits you to use this raw representation directly, but when using plain arrays you need to permute the dimensions of the raw data array.  The convention that a  m x n x 3  array implies RGB is also problematic for anyone doing 3d imaging, and can result in hard-to-find bugs when the third dimension happens to be of size 3. For 3d imaging, the use of an Image type\u2013-perhaps converting Arrays via  grayim \u2013-is highly recommended.  The conventions for plain arrays are \"baked in\" via a few simple utility functions in the file  core.jl ; if you really need to use plain arrays but want to work with different conventions, you can (locally) change these defaults with just a few lines. Algorithms which have been written generically should continue to work.  However, a more flexible approach is to use one of the self-documenting image types.", 
            "title": "Plain arrays"
        }, 
        {
            "location": "/core/#image-types", 
            "text": "All image types should descend from  AbstractImage , an abstract base type used to indicate that an array is to be interpreted as an image. If you're writing a custom image type, it is more likely that you'll want to derive from either  AbstractImageDirect  or  AbstractImageIndexed . The former is for direct images (where intensity at a pixel is represented directly), the latter for indexed images (where intensity is looked up in a colormap table).  In practice, it is assumed that  AbstractImages  have at least two fields, called  data  and  properties . (In code, you should not use these directly, instead using the functions  data  and  properties  to extract these.)  These are the only two fields in the first concrete image type, called  Image :  type Image{T,N,A :AbstractArray}  : AbstractImageDirect{T,N}\n    data::A\n    properties::Dict{String,Any}\nend  data  stores the actual image data, and is an  AbstractArray . This fact alone is the basis for a great deal of customizability:  data  might be a plain  Array  stored in memory, a  SubArray , a memory-mapped array (which is still just an  Array ), a custom type that stores additional information about \"missing data\" (like bad pixels or dropped frames), or a custom type that seamlessly presents views of a large number of separate files.  One concrete example in the Images codebase is the color  Overlay   type .  If you have a suitably-defined  AbstractArray  type, you can probably use  Image  without needing to create alternative  AbstractImageDirect  types.  properties  is a dictionary, with  String  keys, that allows you to annotate images. More detail about this point can be found below.  The only other concrete image type is for indexed images:  type ImageCmap{T,N,A :AbstractArray,C :AbstractArray}  : AbstractImageIndexed{T,N}\n    data::A\n    cmap::C\n    properties::Dict{String,Any}\nend  The  data  array here just encodes the index used to look up the color in the  cmap  field.", 
            "title": "Image types"
        }, 
        {
            "location": "/core/#addressing-image-data", 
            "text": "For any valid image type,  data(img)  returns the array that corresponds to the image.  This works when  img  is a plain  Array  (in which case no operation is performed) as well as for an  Image  (in which case it returns  img.data ). For some image formats, Images.jl may interpret raw data with the  FixedPointNumbers  package. The function  raw(img)  can be used to recover the buffer in its raw format (e.g.  UInt8 ). This is our first example of how to write generic algorithms.  If  img  is an  Image , then  img[i,j]  looks up the value  img.data[i,j] . Assignment,  sub , and  slice  work similarly. In other words, for indexing an  Image  works just as if you were using plain arrays.  If you load your image data using Image's  imread , note that the storage order is not changed from the on-disk representation. Therefore, a 2D RGB image will most likely be stored in color-horizontal-vertical order, meaning that a pixel at  (x,y)  is accessed as  img[x,y] . Note that this is quite different from Matlab's default representation.  If you are indexing over an extended region and want to get back an  Image , rather than a value or an  Array , then you will want to use  getindexim ,  subim , and  sliceim . For the first two, the resulting image will share everything but the  data  field with the original image; if you make modifications in one, the other will also be affected. For  sliceim , because it can change the dimensionality some adjustments to  properties  are needed; in this case a copy is made.  One of the properties (see below) that you can grant to images is  spatialorder , which provides a name for each spatial dimension in the image. Using this feature, you can cut out regions or slices from images in the following ways:  A = img[ x , 200:400,  y , 500:700]\nimgs = sliceim(img,  z , 14)      # cuts out the 14th frame in a stack  These routines \"do the right thing\" no matter what storage order is being used.", 
            "title": "Addressing image data"
        }, 
        {
            "location": "/core/#image-properties-and-accessor-functions", 
            "text": "The  properties  dictionary can contain any information you want to store along with your images. Typically, each property is also affiliated with an accessor function of the same name.  Let's illustrate this with one of the default properties,  \"colorspace\" . The value of this property is a string, such as  \"RGB\"  or  \"Gray\"  or  \"HSV\" . You can extract the value of this field using a function:  cs = colorspace(img)  The reason to have a function, rather than just looking it up in the  properties  dictionary, is that we can provide defaults. For example, images represented as plain  Array s don't have a  properties  dictionary; if we are to write generic code, we don't want to have to wonder whether this information is available. So for plain arrays, there are a number of defaults specified for the output of the  colorspace  function, depending on the element type and size of the array. Likewise, images stored as  Color  arrays have no need of a  \"colorspace\"  property, because the colorspace is encoded in the type parameters.  Here is a list of the properties supported in  core.jl :   colorspace : \"RGB\", \"RGBA\", \"Gray\", \"Binary\", \"24bit\", \"Lab\", \"HSV\", etc.  If   your image is represented as a Color array, you cannot override that   choice by specifying a  colorspace  property.  (Use  reinterpret  if you want   to change the interpretation without changing the raw values.)  colordim : the array dimension used to store color information, or 0 if there   is no dimension corresponding to color  timedim : the array dimension used for time (i.e., sequence), or 0 for single   images  scalei : a property that controls default contrast scaling upon display.   This should be a    MapInfo    value, to be used for setting the contrast upon display. In the absence of   this property, the range 0 to 1 will be used.  pixelspacing : the spacing between adjacent pixels along spatial dimensions  spacedirections : more detailed information about the orientation of array   axes relative to an external coordinate system (see the    function reference ).  spatialorder : a string naming each spatial dimension of the array, in the   storage order of the data array.  Names can be arbitrary, but the choices \"x\"   and \"y\" have special meaning (horizontal and vertical, respectively,   irrespective of storage order).  If supplied, you must have one entry per   spatial dimension.   If you specify their values in the  properties  dictionary, your values will be used; if not, hopefully-reasonable defaults will be chosen.  Naturally, you can add whatever additional properties you want: you could add the date/time at which the image was captured, the patient ID, etc. The main point of having a properties dictionary, rather than a type with fixed fields, is the flexibility of adding whatever metadata you find to be useful.", 
            "title": "Image properties and accessor functions"
        }, 
        {
            "location": "/core/#writing-generic-algorithms", 
            "text": "Let's say you have an algorithm implemented for  Array s, and you want to extend it to work on  Image  types. Let's consider the example of a hypothetical  imfilter , written to perform kernel-based filtering in arbitrary dimensions. Let's say your  imfilter  looks like this:  function imfilter{T,N}(A::Array{T,N}, kernel::Array{T,N}, options...)  The first step might be to simply provide a version for  AbstractImage  types:  function imfilter{T,N}(img::AbstractImage{T,N}, kernel::Array{T,N}, options...)\n    out = imfilter(data(img), kernel, options...)\n    shareproperties(img, out)\nend  Now let's say you additionally want to allow the user to filter color images\u2013-where one dimension of the array is used to encode color\u2013-with a filter of dimension  N-1  applied to each color channel separately. We can implement this version simultaneously for both  Image  types and other array types as follows:  function imfilter{T,N,N1}(img::AbstractArray{T,N}, kernel::Array{T,N1}, options...)\n    cd = colordim(img)\n    if N1 != N - (cd != 0)\n        error( kernel has the wrong dimensionality )\n    end\n    out = similar(img)\n    for i = size(img, cd)\n        imsl = img[ color , i]\n        outsl = slice(out,  color , i)\n        copy!(outsl, imfilter(imsl, kernel, options...))\n    end\n    out\nend  There are other ways to achieve a similar effect; if you examine the actual implementation of  imfilter , you'll see that the kernel is reshaped to be commensurate with the data array.  These solutions work no matter which dimension is used to store color, a feat that would be essentially impossible to achieve robustly in a generic algorithm if we didn't exploit metadata. Note also that if the user supplies an  Array , s/he will get an  Array  back, and if using an  Image  will get an  Image  back with properties inherited from  img .  Naturally, you can find other examples of generic implementations throughout the source code of  Images .", 
            "title": "Writing generic algorithms"
        }, 
        {
            "location": "/overlays/", 
            "text": "Overlays\n\n\nFrequently one wants to combine two (or more) grayscale images into a single colorized image.  \nImages\n defines an \nAbstractArray\n type, \nOverlay\n, making this straightforward.\n\n\nTo create an overlay, use the following syntax:\n\n\nO = Overlay((gray1,gray2,...), (color1,color2,...), (clim1,clim2,...))\n\n\n\n\nHere \ngray1\n and \ngray2\n are the arrays representing individual \"channels\" of information, each equivalent to a grayscale image.  \ncolor1\n and \ncolor2\n are the \nColors\n that will be used for the corresponding grayscale arrays; for example, to put \ngray1\n in the red channel, you'd use \ncolor1 = RGB(1,0,0)\n.  (You can choose any RGB value you want, it doesn't have to be a \"pure\" RGB channel.)  Finally, \nclim1\n and \nclim2\n represent the intensity-scaling applied to each image; setting \nclim1 = (400,3000)\n would send any values in \ngray1\n less than 400 to black, and any values bigger than 3000 to red, with other values between encoded linearly.\n\n\nOnce constructed, an \nOverlay\n acts as an \nArray{RGB}\n. You can embed it in an \nImage\n or just use it directly.", 
            "title": "Overlays"
        }, 
        {
            "location": "/overlays/#overlays", 
            "text": "Frequently one wants to combine two (or more) grayscale images into a single colorized image.   Images  defines an  AbstractArray  type,  Overlay , making this straightforward.  To create an overlay, use the following syntax:  O = Overlay((gray1,gray2,...), (color1,color2,...), (clim1,clim2,...))  Here  gray1  and  gray2  are the arrays representing individual \"channels\" of information, each equivalent to a grayscale image.   color1  and  color2  are the  Colors  that will be used for the corresponding grayscale arrays; for example, to put  gray1  in the red channel, you'd use  color1 = RGB(1,0,0) .  (You can choose any RGB value you want, it doesn't have to be a \"pure\" RGB channel.)  Finally,  clim1  and  clim2  represent the intensity-scaling applied to each image; setting  clim1 = (400,3000)  would send any values in  gray1  less than 400 to black, and any values bigger than 3000 to red, with other values between encoded linearly.  Once constructed, an  Overlay  acts as an  Array{RGB} . You can embed it in an  Image  or just use it directly.", 
            "title": "Overlays"
        }, 
        {
            "location": "/function_reference/", 
            "text": "Function Reference\n\n\nBelow, \n[]\n in an argument list means an optional argument.\n\n\n\n\nImage construction\n\n\n#\n\n\nImages.Image\n \n \nType\n.\n\n\nImage(data, [properties])\nImage(data, prop1=val1, prop2=val2, ...)\n\n\n\n\ncreates a new \"direct\" image, one in which the values in \ndata\n correspond to the pixel values. In contrast with \nconvert\n, \ngrayim\n and \ncolorim\n, this does not permute the data array or attempt to guess any of the \nproperties\n. If \ndata\n encodes color information along one of the dimensions of the array (as opposed to using a \nColor\n array, from the \nColors.jl\n package), be sure to specify the \n\"colordim\"\n and \n\"colorspace\"\n in \nproperties\n.\n\n\n#\n\n\nImages.ImageCmap\n \n \nType\n.\n\n\nImageCmap(data, cmap, [properties])\n\n\n\n\ncreates an indexed (colormap) image.\n\n\nconvert(Image, A)\nconvert(Array, img)\nconvert(Image{HSV}, img)\n\n\n\n\nThe first creates a 2d image from an array, setting up default properties. The data array is assumed to be in \"vertical-major\" order, and an m-by-n-by-3 array will be assumed to encode color along its third dimension.\n\n\nconvert(Array, img)\n works in the opposite direction, permuting dimensions (if needed) to put it in Matlab-standard storage order.\n\n\nThe third syntax allows you to convert from one colorspace to another.\n\n\n#\n\n\nImages.grayim\n \n \nFunction\n.\n\n\nimg = grayim(A)\n\n\n\n\ncreates a 2d or 3d \nspatial\n grayscale Image from an AbstractArray \nA\n, assumed to be in \"horizontal-major\" order (and without permuting any dimensions). If you are working with 3d grayscale images, usage of this function is strongly recommended. This can fix errors like any of the following:\n\n\nERROR: Wrong number of spatial dimensions for plain Array, use an AbstractImage type\nERROR: Cannot infer colorspace of Array, use an AbstractImage type\nERROR: Cannot infer pixelspacing of Array, use an AbstractImage type\n\n\n\n\nThe main reason for such errors\u2013-and the reason that \ngrayim\n is recommended\u2013-is the Matlab-derived convention that a \nm x n x 3\n array is to be interpreted as RGB.  One might then say that an \nm x n x k\n array, for \nk\n different from 3, could be interpreted as grayscale. However, this would lead to difficult-to-track-down surprises on the day where \nk\n happened to be 3 for your grayscale image.\n\n\nSee also: \ncolorim\n, \nImage\n, \nconvert(Image, A)\n.\n\n\n#\n\n\nImages.colorim\n \n \nFunction\n.\n\n\nimg = colorim(A, [colorspace])\n\n\n\n\nCreates a 2d color image from an AbstractArray \nA\n, auto-detecting which of the first or last dimension encodes the color and choosing between \"horizontal-\" and \"vertical-major\" accordingly. \ncolorspace\n defaults to \n\"RGB\"\n but could also be e.g. \n\"Lab\"\n or \n\"HSV\"\n.  If the array represents a 4-channel image, the \ncolorspace\n option is mandatory since there is no way to automatically distinguish between \n\"ARGB\"\n and \n\"RGBA\"\n.  If both the first and last dimensions happen to be of size 3 or 4, it is impossible to guess which one represents color and thus an error is generated.  Thus, if your code needs to be robust to arbitrary-sized images, you should use the \nImage\n constructor directly.\n\n\nSee also: \ngrayim\n, \nImage\n, \nconvert(Image{RGB}, A)\n.\n\n\n#\n\n\nImages.copyproperties\n \n \nFunction\n.\n\n\nimgnew = copyproperties(img, data)\n\n\n\n\nCreates a new image from the data array \ndata\n, copying the properties from Image \nimg\n.\n\n\n#\n\n\nImages.shareproperties\n \n \nFunction\n.\n\n\nimgnew = shareproperties(img, data)\n\n\n\n\nCreates a new image from the data array \ndata\n, \nsharing\n the properties of Image \nimg\n. Any modifications made to the properties of one will affect the other.\n\n\n#\n\n\nBase.similar\n \n \nFunction\n.\n\n\nsimilar(array, [element_type=eltype(array)], [dims=size(array)])\n\n\n\n\nCreate an uninitialized mutable array with the given element type and size, based upon the given source array. The second and third arguments are both optional, defaulting to the given array's \neltype\n and \nsize\n. The dimensions may be specified either as a single tuple argument or as a series of integer arguments.\n\n\nCustom AbstractArray subtypes may choose which specific array type is best-suited to return for the given element type and dimensionality. If they do not specialize this method, the default is an \nArray(element_type, dims...)\n.\n\n\nFor example, \nsimilar(1:10, 1, 4)\n returns an uninitialized \nArray{Int,2}\n since ranges are neither mutable nor support 2 dimensions:\n\n\njulia\n similar(1:10, 1, 4)\n1x4 Array{Int64,2}:\n 4419743872  4374413872  4419743888  0\n\n\n\n\nConversely, \nsimilar(trues(10,10), 2)\n returns an uninitialized \nBitVector\n with two elements since \nBitArray\ns are both mutable and can support 1-dimensional arrays:\n\n\njulia\n similar(trues(10,10), 2)\n2-element BitArray{1}:\n false\n false\n\n\n\n\nSince \nBitArray\ns can only store elements of type \nBool\n, however, if you request a different element type it will create a regular \nArray\n instead:\n\n\njulia\n similar(falses(10), Float64, 2, 4)\n2x4 Array{Float64,2}:\n 2.18425e-314  2.18425e-314  2.18425e-314  2.18425e-314\n 2.18425e-314  2.18425e-314  2.18425e-314  2.18425e-314\n\n\n\n\n#\n\n\nImages.Overlay\n \n \nType\n.\n\n\nA = Overlay(channels, colors)\nA = Overlay(channels, colors, clim)\nA = Overlay(channels, colors, mapi)\n\n\n\n\nCreate an \nOverlay\n array from grayscale channels.  \nchannels = (channel1, channel2, ...)\n, \ncolors\n is a vector or tuple of \nColor\ns, and \nclim\n is a vector or tuple of min/max values, e.g., \nclim = ((min1,max1),(min2,max2),...)\n. Alternatively, you can supply a list of \nMapInfo\n objects.\n\n\nSee also: \nOverlayImage\n.\n\n\n#\n\n\nImages.OverlayImage\n \n \nFunction\n.\n\n\nOverlayImage\n is identical to \nOverlay\n, except that it returns an Image.\n\n\n\n\nAccessing image data\n\n\n#\n\n\nImages.data\n \n \nFunction\n.\n\n\nA = data(img)\n\n\n\n\nreturns a reference \nA\n to the array data in \nimg\n. It allows you to use algorithms specialized for particular \nAbstractArray\n types on \nImage\n types. This works for both \nAbstractImage\ns and \nAbstractArray\ns (for the latter it just returns the input), so is a \"safe\" component of any algorithm.\n\n\nFor algorithms written to accept arbitrary \nAbstractArrays\n, this function is not needed.\n\n\n#\n\n\nImages.raw\n \n \nFunction\n.\n\n\nimgraw = raw(img)\n\n\n\n\nreturns a reference to the array data in raw (machine-native) storage format. This is particularly useful when Images.jl wraps image data in a \nFixedPointNumbers\n type, and raw data access is desired. For example\n\n\nimg = load(\nsomeimage.tif\n)\ntypeof( data(img) )  # return Array{UFixed{UInt8,8},2}\ntypeof( raw(img) )   # returns Array{UInt8,2}\n\n\n\n\n#\n\n\nImages.separate\n \n \nFunction\n.\n\n\nimgs = separate(img)\n separates the color channels of \nimg\n, for example returning an \nm-by-n-by-3\n array from an \nm-by-n\n array of \nRGB\n.\n\n\nimg\n\n\nimg[i, j, k,...]\nimg[\nx\n, 100:200, \ny\n, 400:600]\n\n\n\n\nreturn image data as an array. The latter syntax allows you to address dimensions by name, irrespective of the storage order. The returned values have the same storage order as the parent.\n\n\n#\n\n\nImages.getindexim\n \n \nFunction\n.\n\n\nimgnew = getindexim(img, i, j, k,...)\nimgnew = getindexim(img, \nx\n, 100:200, \ny\n, 400:600)\n\n\n\n\nreturn a new Image \nimgnew\n, copying (and where necessary modifying) the properties of \nimg\n.  This is in contrast with \nimg[i, j, k...]\n, which returns an \nArray\n.\n\n\nsub and slice\n\n\nsub(img, i, j, k, ...)\nsub(img, \nx\n, 100:200, \ny\n, 400:600)\nslice(img, i, j, k, ...)\nslice(img, \nx\n, 15, \ny\n, 400:600)\n\n\n\n\nreturns a \nSubArray\n of image data, with the ordinary meanings of \nsub\n and \nslice\n.\n\n\nsubim and sliceim\n\n\nsubim(img, i, j, k, ...)\nsubim(img, \nx\n, 100:200, \ny\n, 400:600)\nsliceim(img, i, j, k, ...)\nsubim(img, \nx\n, 15, \ny\n, 400:600)\n\n\n\n\nreturns an \nImage\n with \nSubArray\n data.\n\n\n\n\nProperties dictionary-like interface\n\n\nUnless specified, these functions work on both plain arrays (when properties can be inferred), and on \nImage\n types.\n\n\nval = img[\npropertyname\n]\nimg[\npropertyname\n] = val\n\n\n\n\nget and set, respectively, the value of a property. These work only for \nImage\n types.\n\n\nhaskey\n\n\nhaskey(img, \npropertyname\n)\n\n\n\n\nTests whether the named property exists. Returns false for \nArray\ns.\n\n\nget\n\n\nget(img, \npropertyname\n, defaultvalue)\n\n\n\n\nGets the named property, or returns the default if not present. For \nArray\n, the default is always returned.\n\n\n\n\nProperties accessor-function interface\n\n\nUnless specified, these functions work on both plain arrays (when properties can be inferred), and on \nImage\n types.\n\n\n#\n\n\nImages.assert2d\n \n \nFunction\n.\n\n\nassert2d(img)\n triggers an error if the image has more than two spatial dimensions or has a time dimension.\n\n\n#\n\n\nImages.assert_scalar_color\n \n \nFunction\n.\n\n\nassert_scalar_color(img)\n triggers an error if the image uses an array dimension to encode color.\n\n\n#\n\n\nImages.assert_timedim_last\n \n \nFunction\n.\n\n\nassert_timedim_last(img)\n triggers an error if the image has a time dimension that is not the last dimension.\n\n\n#\n\n\nImages.assert_xfirst\n \n \nFunction\n.\n\n\nassert_xfirst(img)\n triggers an error if the first spatial dimension is not \n\"x\"\n.\n\n\n#\n\n\nImages.colordim\n \n \nFunction\n.\n\n\ndim = colordim(img)\n returns the dimension used to encode color, or 0 if no dimension of the array is used for color. For example, an \nArray\n of size \n(m, n, 3)\n would result in 3, whereas an \nArray\n of \nRGB\n colorvalues would yield 0.\n\n\nSee also: \nncolorelem\n, \ntimedim\n.\n\n\n#\n\n\nImages.colorspace\n \n \nFunction\n.\n\n\ncs = colorspace(img)\n returns a string specifying the colorspace representation of the image.\n\n\n#\n\n\nImages.coords_spatial\n \n \nFunction\n.\n\n\nc = coords_spatial(img)\n returns a vector listing the spatial dimensions of the image. For example, an \nArray\n of size \n(m,n,3)\n would return \n[1,2]\n.\n\n\nSee also: \nspatialorder\n.\n\n\n#\n\n\nGraphics.height\n \n \nFunction\n.\n\n\nh = height(img)\n returns the vertical size of the image, regardless of storage order. By default horizontal corresponds to dimension \n\"y\"\n, but see \nspatialpermutation\n for other options.\n\n\n#\n\n\nImages.isdirect\n \n \nFunction\n.\n\n\nisdirect(img)\n returns true if \nimg\n encodes its values directly, rather than via an indexed colormap.\n\n\n#\n\n\nImages.isxfirst\n \n \nFunction\n.\n\n\ntf = isxfirst(img)\n tests whether the first spatial dimension is \n\"x\"\n.\n\n\nSee also: \nisyfirst\n, \nassert_xfirst\n.\n\n\n#\n\n\nImages.isyfirst\n \n \nFunction\n.\n\n\ntf = isyfirst(img)\n tests whether the first spatial dimension is \n\"y\"\n.\n\n\nSee also: \nisxfirst\n, \nassert_yfirst\n.\n\n\n#\n\n\nImages.pixelspacing\n \n \nFunction\n.\n\n\nps = pixelspacing(img)\n\n\n\n\nReturns a vector \nps\n containing the spacing between adjacent pixels along each dimension. If this property is not available, it will be computed from \n\"spacedirections\"\n if present; otherwise it defaults to \nones(sdims(img))\n. If desired, you can set this property in terms of physical \nunits\n.\n\n\nSee also: \nspacedirections\n.\n\n\n#\n\n\nImages.spacedirections\n \n \nFunction\n.\n\n\nsd = spacedirections(img)\n\n\n\n\nReturns a vector-of-vectors \nsd\n, each \nsd[i]\nindicating the displacement between adjacent pixels along spatial axis \ni\n of the image array, relative to some external coordinate system (\"physical coordinates\").  For example, you could indicate that a photograph was taken with the camera tilted 30-degree relative to vertical using\n\n\nimg[\nspacedirections\n] = [[0.866025,-0.5],[0.5,0.866025]]\n\n\n\n\nIf not specified, it will be computed from \npixelspacing(img)\n, placing the spacing along the \"diagonal\".  If desired, you can set this property in terms of physical \nunits\n.\n\n\nSee also: \npixelspacing\n.\n\n\n#\n\n\nImages.nimages\n \n \nFunction\n.\n\n\nn = nimages(img)\n returns the number of time-points in the image array. This is safer than \nsize(img, \"t\")\n because it also works for plain \nAbstractArray\n types.\n\n\n#\n\n\nImages.sdims\n \n \nFunction\n.\n\n\nn = sdims(img)\n is similar to \nndims\n, but it returns just the number of \nspatial\n dimensions in the image array (excluding color and time).\n\n\nsize\n\n\nsize(img, 2)\nsize(img, \nt\n)\n\n\n\n\nObtains the size of the specified dimension, even for dimensions specified by name. See also \nnimages\n, \nsize_spatial\n, \nwidth\n, \nheight\n, and \nwidthheight\n.\n\n\n#\n\n\nImages.size_spatial\n \n \nFunction\n.\n\n\nssz = size_spatial(img)\n\n\n\n\nReturns a tuple listing the sizes of the spatial dimensions of the image. For example, an \nArray\n of size \n(m,n,3)\n would return \n(m,n)\n.\n\n\nSee also: \nnimages\n, \nwidth\n, \nheight\n, \nwidthheight\n.\n\n\n#\n\n\nImages.spatialorder\n \n \nFunction\n.\n\n\nso = spatialorder(img)\nso = spatialorder(ImageType)\n\n\n\n\nReturns the storage order of the \nspatial\n coordinates of the image, e.g., \n[\"y\", \"x\"]\n. The second version works on a type, e.g., \nMatrix\n.\n\n\nSee also: \nstorageorder\n, \ncoords_spatial\n, \ntimedim\n, and \ncolordim\n.\n\n\norder = spatialorder(img)\norder = spatialorder(ImageType)\n\n\n\n\nReturns the storage order of the \nspatial\n coordinates of the image, e.g., \n[\"y\", \"x\"]\n. The second version works on a type, e.g., \nMatrix\n. See \nstorageorder\n, \ntimedim\n, and \ncolordim\n for related properties.\n\n\n#\n\n\nImages.spatialpermutation\n \n \nFunction\n.\n\n\np = spatialpermutation(to, img)\n\n\n\n\nCalculates the \nspatial\n permutation needed to convert the spatial dimensions to a given order. This is probably easiest to understand by examples: for an \nArray\n \nA\n of size \n(m,n,3)\n, \nspatialorder(A)\n would yield \n[\"y\", \"x\"]\n, so \nspatialpermutation([\"y\", \"x\"], A) = [1,2]\n and \nspatialpermutation([\"x\", \"y\"], A) = [2,1]\n.  For an image type, here's a demonstration:\n\n\njulia\n Aimg = convert(Image, A)\nRGB Image with:\n  data: 4x5x3 Array{Float64,3}\n  properties:\n    colordim: 3\n    spatialorder:  y x\n    colorspace: RGB\n\njulia\n Ap = permutedims(Aimg, [3, 1, 2])\nRGB Image with:\n  data: 3x4x5 Array{Float64,3}\n  properties:\n    colordim: 1\n    spatialorder:  y x\n    colorspace: RGB\n\njulia\n spatialpermutation([\nx\n,\ny\n], Ap)\n2-element Array{Int64,1}:\n 2\n 1\n\n\n\n\n#\n\n\nImages.spatialproperties\n \n \nFunction\n.\n\n\nsp = spatialproperties(img)\n\n\n\n\nReturns all properties whose values are of the form of an array or tuple, with one entry per spatial dimension. If you have a custom type with additional spatial properties, you can set \nimg[\"spatialproperties\"] = [\"property1\", \"property2\", ...]\n. An advantage is that functions that change spatial dimensions, like \npermutedims\n and \nslice\n, will also adjust the properties. The default is \n[\"spatialorder\", \"pixelspacing\"]\n; however, if you override the setting then these are not included automatically (you'll want to do so manually, if applicable).\n\n\n#\n\n\nImages.storageorder\n \n \nFunction\n.\n\n\nso = storageorder(img)\n\n\n\n\nReturns the complete storage order of the image array, including \n\"t\"\n for time and \n\"color\"\n for color.\n\n\nSee also: \nspatialorder\n, \ncolordim\n, \ntimedim\n.\n\n\n#\n\n\nImages.timedim\n \n \nFunction\n.\n\n\ndim = timedim(img)\n returns the dimension used to represent time, or 0 if this is a single image.\n\n\nSee also: \nnimages\n, \ncolordim\n.\n\n\n#\n\n\nGraphics.width\n \n \nFunction\n.\n\n\nw = width(img)\n returns the horizontal size of the image, regardless of storage order. By default horizontal corresponds to dimension \n\"x\"\n, but see \nspatialpermutation\n for other options.\n\n\n#\n\n\nImages.widthheight\n \n \nFunction\n.\n\n\nw, h = widthheight(img)\n returns the width and height of an image, regardless of storage order.\n\n\nSee also: \nwidth\n, \nheight\n.\n\n\n\n\nElement transformation and intensity scaling\n\n\nMany images require some type of transformation before you can use or view them. For example, visualization libraries work in terms of 8-bit data, so if you're using a 16-bit scientific camera, your image values will need to be scaled before display.\n\n\nOne can directly rescale the pixel intensities in the image array.  In general, element-wise transformations are handled by \nmap\n or \nmap!\n, where the latter is used when you want to provide a pre-allocated output.  You can use an anonymous function of your own design, or, if speed is paramount, the \"anonymous functions\" of the \nFastAnonymous\n package.\n\n\nImages also supports \"lazy transformations.\" When loading a very large image, (e.g., loaded by memory-mapping) you may use or view just a small portion of it. In such cases, it would be quite wasteful to force transformation of the entire image, and indeed on might exhaust available memory or need to write a new file on disk.  \nImages\n supports lazy-evaluation scaling through the \nMapInfo\n abstract type.  The basic syntax is\n\n\nvalout = map(mapi::MapInfo, valin)\n\n\n\n\nHere \nval\n can refer to a single pixel's data, or to the entire image array. The \nmapi\n input is a type that determines how the input value is scale and converted to a new type.\n\n\n\n\nMapInfo\n\n\nHere is how to directly construct the major concrete \nMapInfo\n types:\n\n\n\n\n\n\nMapNone(T)\n, indicating that the only form of scaling is conversion   to type T.  This can throw an error if a value \nx\n cannot be   represented as an object of type \nT\n, e.g., \nmap(MapNone{U8}, 1.2)\n.\n\n\n\n\n\n\nClampMin(T, minvalue)\n, \nClampMax(T, maxvalue)\n, and   \nClampMinMax(T, minvalue, maxvalue)\n create \nMapInfo\n objects that   clamp pixel values at the specified min, max, and min/max values,   respectively, before converting to type \nT\n. Clamping is equivalent   to \nclampedval = min(max(val, minvalue), maxvalue)\n.\n\n\n\n\n\n\nBitShift(T, N)\n or \nBitShift{T,N}()\n, for scaling by bit-shift operators.   \nN\n specifies the number of bits to right-shift by.  For example you could   convert a 14-bit image to 8-bits using \nBitShift(Uint8, 6)\n.  In general this   will be faster than using multiplication.\n\n\n\n\n\n\nScaleMinMax(T, min, max, [scalefactor])\n clamps the image at the specified   min/max values, subtracts the min value, scales the result by multiplying by   \nscalefactor\n, and finally converts the type.  If \nscalefactor\n is not   specified, it defaults to scaling the range \n[min,max]\n to \n[0,1]\n.\n\n\n\n\n\n\nScaleAutoMinMax(T)\n will cause images to be dynamically scaled to their   specific min/max values, using the same algorithm for \nScaleMinMax\n. When   displaying a movie, the min/max will be recalculated for each frame, so this   can result in inconsistent contrast scaling.\n\n\n\n\n\n\nScaleSigned(T, scalefactor)\n multiplies the image by the scalefactor, then   clamps to the range \n[-1,1]\n. If \nT\n is a floating-point type, it stays in   this representation.  If \nT\n is \nRGB24\n or \nRGB{UFixed8}\n, then it is encoded   as a magenta (positive)/green (negative) image.\n\n\n\n\n\n\nThere are also convenience functions:\n\n\n#\n\n\nImages.imstretch\n \n \nFunction\n.\n\n\nimgs = imstretch(img, m, slope)\n enhances or reduces (for slope \n 1 or \n 1, respectively) the contrast near saturation (0 and 1). This is essentially a symmetric gamma-correction. For a pixel of brightness \np\n, the new intensity is \n1/(1+(m/(p+eps))^slope)\n.\n\n\nThis assumes the input \nimg\n has intensities between 0 and 1.\n\n\n#\n\n\nImages.sc\n \n \nFunction\n.\n\n\nimgsc = sc(img)\nimgsc = sc(img, min, max)\n\n\n\n\nApplies default or specified \nScaleMinMax\n mapping to the image.\n\n\n#\n\n\nImages.MapInfo\n \n \nType\n.\n\n\nMapInfo{T}\n is an abstract type that encompasses objects designed to perform intensity or color transformations on pixels.  For example, before displaying an image in a window, you might need to adjust the contrast settings; \nMapInfo\n objects provide a means to describe these transformations without calculating them immediately.  This delayed execution can be useful in many contexts.  For example, if you want to display a movie, it would be quite wasteful to have to first transform the entire movie; instead, \nMapInfo\n objects allow one to specify a transformation to be performed on-the-fly as particular frames are displayed.\n\n\nYou can create your own custom \nMapInfo\n objects. For example, given a grayscale image, you could color \"saturated\" pixels red using\n\n\nimmutable ColorSaturated{C\n:AbstractRGB} \n: MapInfo{C}\nend\n\nBase.map{C}(::ColorSaturated{C}, val::Union{Number,Gray}) = ifelse(val == 1, C(1,0,0), C(val,val,val))\n\nimgc = map(ColorSaturated{RGB{U8}}(), img)\n\n\n\n\nFor pre-defined types see \nMapNone\n, \nBitShift\n, \nClampMinMax\n, \nScaleMinMax\n, \nScaleAutoMinMax\n, and \nScaleSigned\n.\n\n\n#\n\n\nImages.mapinfo\n \n \nFunction\n.\n\n\nmapi = mapinf(T, img)\n returns a \nMapInfo\n object that is deemed appropriate for converting pixels of \nimg\n to be of type \nT\n. \nT\n can either be a specific type (e.g., \nRGB24\n), or you can specify an abstract type like \nClamp\n and it will return one of the \nClamp\n family of \nMapInfo\n objects.\n\n\nYou can define your own rules for \nmapinfo\n.  For example, the \nImageMagick\n package defines methods for how pixels values should be converted before saving images to disk.\n\n\n\n\nColor conversion\n\n\nconvert\n\n\nconvert(Image{Color}, img)\n\n\n\n\nas described above. Use \nconvert(Image{Gray}, img)\n to calculate a grayscale representation of a color image using the \nRec 601 luma\n.\n\n\nmap\n\n\nmap(mapi, img)\nmap!(mapi, dest, img)\n\n\n\n\ncan be used to specify both the form of the result and the algorithm used.\n\n\n\n\nImage I/O\n\n\nImage loading and saving is handled by the \nFileIO\n package.\n\n\n\n\nImage algorithms\n\n\nYou can perform arithmetic with \nImage\ns and \nColor\ns. Algorithms also include the following functions:\n\n\n\n\nLinear filtering and padding\n\n\nimfilter\nimfilter!\nimfilter_fft\nimfilter_gaussian\nimfilter_LoG\nimgradients\nmagnitude\nphase\norientation\nmagnitude_phase\nimedge\nthin_edges\nforwarddiffx\nforwarddiffy\nbackdiffx\nbackdiffy\npadarray\n\n\n\n\n\n\nFeature Extraction\n\n\n#\n\n\nImages.blob_LoG\n \n \nFunction\n.\n\n\nblob_LoG(img, sigmas) -\n Vector{Tuple}\n\n\nFind \"blobs\" in an N-D image using Lapacian of Gaussians at the specifed sigmas.  Returned are the local maxima's heights, radii, and spatial coordinates.\n\n\nSee Lindeberg T (1998), \"Feature Detection with Automatic Scale Selection\", International Journal of Computer Vision, 30(2), 79\u2013116.\n\n\nNote that only 2-D images are currently supported due to a limitation of \nimfilter_LoG\n.\n\n\n#\n\n\nImages.findlocalmaxima\n \n \nFunction\n.\n\n\nfindlocalmaxima(img, [region, edges]) -\n Vector{Tuple}\n\n\nReturns the coordinates of elements whose value is larger than all of their immediate neighbors.  \nregion\n is a list of dimensions to consider.  \nedges\n is a boolean specifying whether to include the first and last elements of each dimension.\n\n\n#\n\n\nImages.findlocalminima\n \n \nFunction\n.\n\n\nLike \nfindlocalmaxima\n, but returns the coordinates of the smallest elements.\n\n\n\n\nExposure\n\n\n#\n\n\nImages.imhist\n \n \nFunction\n.\n\n\nedges, count = imhist(img, nbins)\nedges, count = imhist(img, nbins, minval, maxval)\n\n\n\n\nGenerates a histogram for the image over nbins spread between \n(minval, maxval]\n. If \nminval\n and \nmaxval\n are not given, then the minimum and maximum values present in the image are taken.\n\n\nedges\n is a vector that specifies how the range is divided; \ncount[i+1]\n is the number of values \nx\n that satisfy \nedges[i] \n= x \n edges[i+1]\n. \ncount[1]\n is the number satisfying \nx \n edges[1]\n, and \ncount[end]\n is the number satisfying \nx \n= edges[end]\n. Consequently, \nlength(count) == length(edges)+1\n.\n\n\n\n\nFiltering kernels\n\n\n#\n\n\nImages.gaussian2d\n \n \nFunction\n.\n\n\nkern = gaussian2d(sigma, filtersize)\n returns a kernel for FIR-based Gaussian filtering.\n\n\nSee also: \nimfilter_gaussian\n.\n\n\n#\n\n\nImages.imaverage\n \n \nFunction\n.\n\n\nkern = imaverage(filtersize)\n constructs a boxcar-filter of the specified size.\n\n\n#\n\n\nImages.imdog\n \n \nFunction\n.\n\n\nkern = imdog(sigma)\n creates a difference-of-gaussians kernel (\nsigma\ns differing by a factor of \nsqrt(2)\n).\n\n\n#\n\n\nImages.imlaplacian\n \n \nFunction\n.\n\n\nkern = imlaplacian(filtersize)\n returns a kernel for laplacian filtering.\n\n\n#\n\n\nImages.imlog\n \n \nFunction\n.\n\n\nkern = imlog(sigma)\n returns a laplacian-of-gaussian kernel.\n\n\n#\n\n\nImages.sobel\n \n \nFunction\n.\n\n\nkern1, kern2 = sobel()\n returns Sobel filters for dimensions 1 and 2 of your image\n\n\n#\n\n\nImages.prewitt\n \n \nFunction\n.\n\n\nkern1, kern2 = prewitt()\n returns Prewitt filters for dimensions 1 and 2 of your image\n\n\n#\n\n\nImages.ando3\n \n \nFunction\n.\n\n\nkern1, kern2 = ando3()\n returns optimal 3x3 filters for dimensions 1 and 2 of your image, as defined in Ando Shigeru, IEEE Trans. Pat. Anal. Mach. Int., vol. 22 no 3, March 2000.\n\n\nSee also: \nando4\n, \nando5\n.\n\n\n#\n\n\nImages.ando4\n \n \nFunction\n.\n\n\nkern1, kern2 = ando4()\n returns optimal 4x4 filters for dimensions 1 and 2 of your image, as defined in Ando Shigeru, IEEE Trans. Pat. Anal. Mach. Int., vol. 22 no 3, March 2000.\n\n\nSee also: \nando4_sep\n, \nando3\n, \nando5\n.\n\n\n#\n\n\nImages.ando5\n \n \nFunction\n.\n\n\nkern1, kern2 = ando5()\n returns optimal 5x5 filters for dimensions 1 and 2 of your image, as defined in Ando Shigeru, IEEE Trans. Pat. Anal. Mach. Int., vol. 22 no 3, March 2000.\n\n\nSee also: \nando5_sep\n, \nando3\n, \nando4\n.\n\n\n\n\nNonlinear filtering and transformation\n\n\n#\n\n\nImages.imROF\n \n \nFunction\n.\n\n\nimgr = imROF(img, lambda, iterations)\n\n\n\n\nPerform Rudin-Osher-Fatemi (ROF) filtering, more commonly known as Total Variation (TV) denoising or TV regularization. \nlambda\n is the regularization coefficient for the derivative, and \niterations\n is the number of relaxation iterations taken. 2d only.\n\n\n#\n\n\nImages.imcorner\n \n \nFunction\n.\n\n\nimge = imcorner(img; [method], [border], [blockSize], [k])\n\n\n\n\nPerforms corner detection, using either the Harris method or the Shi-Tomasi method. \nmethod\n is \n'harris'\n (default) or \n'shi-tomasi'\n. \n'border'\n is the border  mode used in computing the gradient (default is \n'replicate'\n). \n'blockSize'\n is the size of the box filter kernel (default of \n3\n). \nk\n is the Harris free parameter  (default of \n0.04\n). It is only used when method is set to \n'harris'\n.\n\n\n\n\nResizing\n\n\n#\n\n\nImages.restrict\n \n \nFunction\n.\n\n\nimgr = restrict(img[, region])\n performs two-fold reduction in size along the dimensions listed in \nregion\n, or all spatial coordinates if \nregion\n is not specified.  It anti-aliases the image as it goes, so is better than a naive summation over 2x2 blocks.\n\n\n\n\nImage statistics\n\n\n#\n\n\nImages.minfinite\n \n \nFunction\n.\n\n\nm = minfinite(A)\n calculates the minimum value in \nA\n, ignoring any values that are not finite (Inf or NaN).\n\n\n#\n\n\nImages.maxfinite\n \n \nFunction\n.\n\n\nm = maxfinite(A)\n calculates the maximum value in \nA\n, ignoring any values that are not finite (Inf or NaN).\n\n\n#\n\n\nImages.maxabsfinite\n \n \nFunction\n.\n\n\nm = maxabsfinite(A)\n calculates the maximum absolute value in \nA\n, ignoring any values that are not finite (Inf or NaN).\n\n\n#\n\n\nImages.meanfinite\n \n \nFunction\n.\n\n\nM = meanfinite(img, region)\n calculates the mean value along the dimensions listed in \nregion\n, ignoring any non-finite values.\n\n\n#\n\n\nImages.ssd\n \n \nFunction\n.\n\n\ns = ssd(A, B)\n computes the sum-of-squared differences over arrays/images A and B\n\n\n#\n\n\nImages.ssdn\n \n \nFunction\n.\n\n\ns = ssdn(A, B)\n computes the sum-of-squared differences over arrays/images A and B, normalized by array size\n\n\n#\n\n\nImages.sad\n \n \nFunction\n.\n\n\ns = sad(A, B)\n computes the sum-of-absolute differences over arrays/images A and B\n\n\n#\n\n\nImages.sadn\n \n \nFunction\n.\n\n\ns = sadn(A, B)\n computes the sum-of-absolute differences over arrays/images A and B, normalized by array size\n\n\n\n\nMorphological operations\n\n\n#\n\n\nImages.dilate\n \n \nFunction\n.\n\n\nimgd = dilate(img, [region])\n\n\n\n\nperform a max-filter over nearest-neighbors. The default is 8-connectivity in 2d, 27-connectivity in 3d, etc. You can specify the list of dimensions that you want to include in the connectivity, e.g., \nregion = [1,2]\n would exclude the third dimension from filtering.\n\n\n#\n\n\nImages.erode\n \n \nFunction\n.\n\n\nimge = erode(img, [region])\n\n\n\n\nperform a min-filter over nearest-neighbors. The default is 8-connectivity in 2d, 27-connectivity in 3d, etc. You can specify the list of dimensions that you want to include in the connectivity, e.g., \nregion = [1,2]\n would exclude the third dimension from filtering.\n\n\n#\n\n\nImages.opening\n \n \nFunction\n.\n\n\nimgo = opening(img, [region])\n performs the \nopening\n morphology operation, equivalent to \ndilate(erode(img))\n. \nregion\n allows you to control the dimensions over which this operation is performed.\n\n\n#\n\n\nImages.closing\n \n \nFunction\n.\n\n\nimgc = closing(img, [region])\n performs the \nclosing\n morphology operation, equivalent to \nerode(dilate(img))\n. \nregion\n allows you to control the dimensions over which this operation is performed.\n\n\n#\n\n\nImages.tophat\n \n \nFunction\n.\n\n\nimgth = tophat(img, [region])\n performs \ntop hat\n of an image, which is defined as the image minus its morphological opening. \nregion\n allows you to control the dimensions over which this operation is performed.\n\n\n#\n\n\nImages.bothat\n \n \nFunction\n.\n\n\nimgbh = bothat(img, [region])\n performs \nbottom hat\n of an image, which is defined as its morphological closing minus the original image. \nregion\n allows you to control the dimensions over which this operation is performed.\n\n\n#\n\n\nImages.morphogradient\n \n \nFunction\n.\n\n\nimgmg = morphogradient(img, [region])\n returns morphological gradient of the image, which is the difference between the dilation and the erosion of a given image. \nregion\n allows you to control the dimensions over which this operation is performed.\n\n\n#\n\n\nImages.morpholaplace\n \n \nFunction\n.\n\n\nimgml = morpholaplace(img, [region])\n performs \nMorphological Laplacian\n of an image, which is defined as the arithmetic difference between the internal and the external gradient. \nregion\n allows you to control the dimensions over which this operation is performed.\n\n\n#\n\n\nImages.label_components\n \n \nFunction\n.\n\n\nlabel = label_components(tf, [connectivity])\nlabel = label_components(tf, [region])\n\n\n\n\nFind the connected components in a binary array \ntf\n. There are two forms that \nconnectivity\n can take:\n\n\n\n\n\n\nIt can be a boolean array of the same dimensionality as \ntf\n, of size 1 or 3 along each dimension. Each entry in the array determines whether a given neighbor is used for connectivity analyses. For example, \nconnectivity = trues(3,3)\n would use 8-connectivity and test all pixels that touch the current one, even the corners.\n\n\n\n\n\n\nYou can provide a list indicating which dimensions are used to determine connectivity. For example, \nregion = [1,3]\n would not test neighbors along dimension 2 for connectivity. This corresponds to just the nearest neighbors, i.e., 4-connectivity in 2d and 6-connectivity in 3d.\n\n\n\n\n\n\nThe default is \nregion = 1:ndims(A)\n.\n\n\nThe output \nlabel\n is an integer array, where 0 is used for background pixels, and each connected region gets a different integer index.\n\n\n#\n\n\nImages.component_boxes\n \n \nFunction\n.\n\n\ncomponent_boxes(labeled_array)\n -\n an array of bounding boxes for each label, including the background label 0\n\n\n#\n\n\nImages.component_lengths\n \n \nFunction\n.\n\n\ncomponent_lengths(labeled_array)\n -\n an array of areas (2D), volumes (3D), etc. for each label, including the background label 0\n\n\n#\n\n\nImages.component_indices\n \n \nFunction\n.\n\n\ncomponent_indices(labeled_array)\n -\n an array of pixels for each label, including the background label 0\n\n\n#\n\n\nImages.component_subscripts\n \n \nFunction\n.\n\n\ncomponent_subscripts(labeled_array)\n -\n an array of pixels for each label, including the background label 0\n\n\n#\n\n\nImages.component_centroids\n \n \nFunction\n.\n\n\ncomponent_centroids(labeled_array)\n -\n an array of centroids for each label, including the background label 0\n\n\n\n\nPhantoms\n\n\n#\n\n\nImages.shepp_logan\n \n \nFunction\n.\n\n\nphantom = shepp_logan(N,[M]; highContrast=true)\n\n\n\n\noutput the NxM Shepp-Logan phantom, which is a standard test image usually used for comparing image reconstruction algorithms in the field of computed tomography (CT) and magnetic resonance imaging (MRI). If the argument M is omitted, the phantom is of size NxN. When setting the keyword argument \n`highConstrast\n to false, the CT version of the phantom is created. Otherwise, the high contrast MRI version is calculated.", 
            "title": "Function Reference"
        }, 
        {
            "location": "/function_reference/#function-reference", 
            "text": "Below,  []  in an argument list means an optional argument.", 
            "title": "Function Reference"
        }, 
        {
            "location": "/function_reference/#image-construction", 
            "text": "#  Images.Image     Type .  Image(data, [properties])\nImage(data, prop1=val1, prop2=val2, ...)  creates a new \"direct\" image, one in which the values in  data  correspond to the pixel values. In contrast with  convert ,  grayim  and  colorim , this does not permute the data array or attempt to guess any of the  properties . If  data  encodes color information along one of the dimensions of the array (as opposed to using a  Color  array, from the  Colors.jl  package), be sure to specify the  \"colordim\"  and  \"colorspace\"  in  properties .  #  Images.ImageCmap     Type .  ImageCmap(data, cmap, [properties])  creates an indexed (colormap) image.  convert(Image, A)\nconvert(Array, img)\nconvert(Image{HSV}, img)  The first creates a 2d image from an array, setting up default properties. The data array is assumed to be in \"vertical-major\" order, and an m-by-n-by-3 array will be assumed to encode color along its third dimension.  convert(Array, img)  works in the opposite direction, permuting dimensions (if needed) to put it in Matlab-standard storage order.  The third syntax allows you to convert from one colorspace to another.  #  Images.grayim     Function .  img = grayim(A)  creates a 2d or 3d  spatial  grayscale Image from an AbstractArray  A , assumed to be in \"horizontal-major\" order (and without permuting any dimensions). If you are working with 3d grayscale images, usage of this function is strongly recommended. This can fix errors like any of the following:  ERROR: Wrong number of spatial dimensions for plain Array, use an AbstractImage type\nERROR: Cannot infer colorspace of Array, use an AbstractImage type\nERROR: Cannot infer pixelspacing of Array, use an AbstractImage type  The main reason for such errors\u2013-and the reason that  grayim  is recommended\u2013-is the Matlab-derived convention that a  m x n x 3  array is to be interpreted as RGB.  One might then say that an  m x n x k  array, for  k  different from 3, could be interpreted as grayscale. However, this would lead to difficult-to-track-down surprises on the day where  k  happened to be 3 for your grayscale image.  See also:  colorim ,  Image ,  convert(Image, A) .  #  Images.colorim     Function .  img = colorim(A, [colorspace])  Creates a 2d color image from an AbstractArray  A , auto-detecting which of the first or last dimension encodes the color and choosing between \"horizontal-\" and \"vertical-major\" accordingly.  colorspace  defaults to  \"RGB\"  but could also be e.g.  \"Lab\"  or  \"HSV\" .  If the array represents a 4-channel image, the  colorspace  option is mandatory since there is no way to automatically distinguish between  \"ARGB\"  and  \"RGBA\" .  If both the first and last dimensions happen to be of size 3 or 4, it is impossible to guess which one represents color and thus an error is generated.  Thus, if your code needs to be robust to arbitrary-sized images, you should use the  Image  constructor directly.  See also:  grayim ,  Image ,  convert(Image{RGB}, A) .  #  Images.copyproperties     Function .  imgnew = copyproperties(img, data)  Creates a new image from the data array  data , copying the properties from Image  img .  #  Images.shareproperties     Function .  imgnew = shareproperties(img, data)  Creates a new image from the data array  data ,  sharing  the properties of Image  img . Any modifications made to the properties of one will affect the other.  #  Base.similar     Function .  similar(array, [element_type=eltype(array)], [dims=size(array)])  Create an uninitialized mutable array with the given element type and size, based upon the given source array. The second and third arguments are both optional, defaulting to the given array's  eltype  and  size . The dimensions may be specified either as a single tuple argument or as a series of integer arguments.  Custom AbstractArray subtypes may choose which specific array type is best-suited to return for the given element type and dimensionality. If they do not specialize this method, the default is an  Array(element_type, dims...) .  For example,  similar(1:10, 1, 4)  returns an uninitialized  Array{Int,2}  since ranges are neither mutable nor support 2 dimensions:  julia  similar(1:10, 1, 4)\n1x4 Array{Int64,2}:\n 4419743872  4374413872  4419743888  0  Conversely,  similar(trues(10,10), 2)  returns an uninitialized  BitVector  with two elements since  BitArray s are both mutable and can support 1-dimensional arrays:  julia  similar(trues(10,10), 2)\n2-element BitArray{1}:\n false\n false  Since  BitArray s can only store elements of type  Bool , however, if you request a different element type it will create a regular  Array  instead:  julia  similar(falses(10), Float64, 2, 4)\n2x4 Array{Float64,2}:\n 2.18425e-314  2.18425e-314  2.18425e-314  2.18425e-314\n 2.18425e-314  2.18425e-314  2.18425e-314  2.18425e-314  #  Images.Overlay     Type .  A = Overlay(channels, colors)\nA = Overlay(channels, colors, clim)\nA = Overlay(channels, colors, mapi)  Create an  Overlay  array from grayscale channels.   channels = (channel1, channel2, ...) ,  colors  is a vector or tuple of  Color s, and  clim  is a vector or tuple of min/max values, e.g.,  clim = ((min1,max1),(min2,max2),...) . Alternatively, you can supply a list of  MapInfo  objects.  See also:  OverlayImage .  #  Images.OverlayImage     Function .  OverlayImage  is identical to  Overlay , except that it returns an Image.", 
            "title": "Image construction"
        }, 
        {
            "location": "/function_reference/#accessing-image-data", 
            "text": "#  Images.data     Function .  A = data(img)  returns a reference  A  to the array data in  img . It allows you to use algorithms specialized for particular  AbstractArray  types on  Image  types. This works for both  AbstractImage s and  AbstractArray s (for the latter it just returns the input), so is a \"safe\" component of any algorithm.  For algorithms written to accept arbitrary  AbstractArrays , this function is not needed.  #  Images.raw     Function .  imgraw = raw(img)  returns a reference to the array data in raw (machine-native) storage format. This is particularly useful when Images.jl wraps image data in a  FixedPointNumbers  type, and raw data access is desired. For example  img = load( someimage.tif )\ntypeof( data(img) )  # return Array{UFixed{UInt8,8},2}\ntypeof( raw(img) )   # returns Array{UInt8,2}  #  Images.separate     Function .  imgs = separate(img)  separates the color channels of  img , for example returning an  m-by-n-by-3  array from an  m-by-n  array of  RGB .  img  img[i, j, k,...]\nimg[ x , 100:200,  y , 400:600]  return image data as an array. The latter syntax allows you to address dimensions by name, irrespective of the storage order. The returned values have the same storage order as the parent.  #  Images.getindexim     Function .  imgnew = getindexim(img, i, j, k,...)\nimgnew = getindexim(img,  x , 100:200,  y , 400:600)  return a new Image  imgnew , copying (and where necessary modifying) the properties of  img .  This is in contrast with  img[i, j, k...] , which returns an  Array .  sub and slice  sub(img, i, j, k, ...)\nsub(img,  x , 100:200,  y , 400:600)\nslice(img, i, j, k, ...)\nslice(img,  x , 15,  y , 400:600)  returns a  SubArray  of image data, with the ordinary meanings of  sub  and  slice .  subim and sliceim  subim(img, i, j, k, ...)\nsubim(img,  x , 100:200,  y , 400:600)\nsliceim(img, i, j, k, ...)\nsubim(img,  x , 15,  y , 400:600)  returns an  Image  with  SubArray  data.", 
            "title": "Accessing image data"
        }, 
        {
            "location": "/function_reference/#properties-dictionary-like-interface", 
            "text": "Unless specified, these functions work on both plain arrays (when properties can be inferred), and on  Image  types.  val = img[ propertyname ]\nimg[ propertyname ] = val  get and set, respectively, the value of a property. These work only for  Image  types.  haskey  haskey(img,  propertyname )  Tests whether the named property exists. Returns false for  Array s.  get  get(img,  propertyname , defaultvalue)  Gets the named property, or returns the default if not present. For  Array , the default is always returned.", 
            "title": "Properties dictionary-like interface"
        }, 
        {
            "location": "/function_reference/#properties-accessor-function-interface", 
            "text": "Unless specified, these functions work on both plain arrays (when properties can be inferred), and on  Image  types.  #  Images.assert2d     Function .  assert2d(img)  triggers an error if the image has more than two spatial dimensions or has a time dimension.  #  Images.assert_scalar_color     Function .  assert_scalar_color(img)  triggers an error if the image uses an array dimension to encode color.  #  Images.assert_timedim_last     Function .  assert_timedim_last(img)  triggers an error if the image has a time dimension that is not the last dimension.  #  Images.assert_xfirst     Function .  assert_xfirst(img)  triggers an error if the first spatial dimension is not  \"x\" .  #  Images.colordim     Function .  dim = colordim(img)  returns the dimension used to encode color, or 0 if no dimension of the array is used for color. For example, an  Array  of size  (m, n, 3)  would result in 3, whereas an  Array  of  RGB  colorvalues would yield 0.  See also:  ncolorelem ,  timedim .  #  Images.colorspace     Function .  cs = colorspace(img)  returns a string specifying the colorspace representation of the image.  #  Images.coords_spatial     Function .  c = coords_spatial(img)  returns a vector listing the spatial dimensions of the image. For example, an  Array  of size  (m,n,3)  would return  [1,2] .  See also:  spatialorder .  #  Graphics.height     Function .  h = height(img)  returns the vertical size of the image, regardless of storage order. By default horizontal corresponds to dimension  \"y\" , but see  spatialpermutation  for other options.  #  Images.isdirect     Function .  isdirect(img)  returns true if  img  encodes its values directly, rather than via an indexed colormap.  #  Images.isxfirst     Function .  tf = isxfirst(img)  tests whether the first spatial dimension is  \"x\" .  See also:  isyfirst ,  assert_xfirst .  #  Images.isyfirst     Function .  tf = isyfirst(img)  tests whether the first spatial dimension is  \"y\" .  See also:  isxfirst ,  assert_yfirst .  #  Images.pixelspacing     Function .  ps = pixelspacing(img)  Returns a vector  ps  containing the spacing between adjacent pixels along each dimension. If this property is not available, it will be computed from  \"spacedirections\"  if present; otherwise it defaults to  ones(sdims(img)) . If desired, you can set this property in terms of physical  units .  See also:  spacedirections .  #  Images.spacedirections     Function .  sd = spacedirections(img)  Returns a vector-of-vectors  sd , each  sd[i] indicating the displacement between adjacent pixels along spatial axis  i  of the image array, relative to some external coordinate system (\"physical coordinates\").  For example, you could indicate that a photograph was taken with the camera tilted 30-degree relative to vertical using  img[ spacedirections ] = [[0.866025,-0.5],[0.5,0.866025]]  If not specified, it will be computed from  pixelspacing(img) , placing the spacing along the \"diagonal\".  If desired, you can set this property in terms of physical  units .  See also:  pixelspacing .  #  Images.nimages     Function .  n = nimages(img)  returns the number of time-points in the image array. This is safer than  size(img, \"t\")  because it also works for plain  AbstractArray  types.  #  Images.sdims     Function .  n = sdims(img)  is similar to  ndims , but it returns just the number of  spatial  dimensions in the image array (excluding color and time).  size  size(img, 2)\nsize(img,  t )  Obtains the size of the specified dimension, even for dimensions specified by name. See also  nimages ,  size_spatial ,  width ,  height , and  widthheight .  #  Images.size_spatial     Function .  ssz = size_spatial(img)  Returns a tuple listing the sizes of the spatial dimensions of the image. For example, an  Array  of size  (m,n,3)  would return  (m,n) .  See also:  nimages ,  width ,  height ,  widthheight .  #  Images.spatialorder     Function .  so = spatialorder(img)\nso = spatialorder(ImageType)  Returns the storage order of the  spatial  coordinates of the image, e.g.,  [\"y\", \"x\"] . The second version works on a type, e.g.,  Matrix .  See also:  storageorder ,  coords_spatial ,  timedim , and  colordim .  order = spatialorder(img)\norder = spatialorder(ImageType)  Returns the storage order of the  spatial  coordinates of the image, e.g.,  [\"y\", \"x\"] . The second version works on a type, e.g.,  Matrix . See  storageorder ,  timedim , and  colordim  for related properties.  #  Images.spatialpermutation     Function .  p = spatialpermutation(to, img)  Calculates the  spatial  permutation needed to convert the spatial dimensions to a given order. This is probably easiest to understand by examples: for an  Array   A  of size  (m,n,3) ,  spatialorder(A)  would yield  [\"y\", \"x\"] , so  spatialpermutation([\"y\", \"x\"], A) = [1,2]  and  spatialpermutation([\"x\", \"y\"], A) = [2,1] .  For an image type, here's a demonstration:  julia  Aimg = convert(Image, A)\nRGB Image with:\n  data: 4x5x3 Array{Float64,3}\n  properties:\n    colordim: 3\n    spatialorder:  y x\n    colorspace: RGB\n\njulia  Ap = permutedims(Aimg, [3, 1, 2])\nRGB Image with:\n  data: 3x4x5 Array{Float64,3}\n  properties:\n    colordim: 1\n    spatialorder:  y x\n    colorspace: RGB\n\njulia  spatialpermutation([ x , y ], Ap)\n2-element Array{Int64,1}:\n 2\n 1  #  Images.spatialproperties     Function .  sp = spatialproperties(img)  Returns all properties whose values are of the form of an array or tuple, with one entry per spatial dimension. If you have a custom type with additional spatial properties, you can set  img[\"spatialproperties\"] = [\"property1\", \"property2\", ...] . An advantage is that functions that change spatial dimensions, like  permutedims  and  slice , will also adjust the properties. The default is  [\"spatialorder\", \"pixelspacing\"] ; however, if you override the setting then these are not included automatically (you'll want to do so manually, if applicable).  #  Images.storageorder     Function .  so = storageorder(img)  Returns the complete storage order of the image array, including  \"t\"  for time and  \"color\"  for color.  See also:  spatialorder ,  colordim ,  timedim .  #  Images.timedim     Function .  dim = timedim(img)  returns the dimension used to represent time, or 0 if this is a single image.  See also:  nimages ,  colordim .  #  Graphics.width     Function .  w = width(img)  returns the horizontal size of the image, regardless of storage order. By default horizontal corresponds to dimension  \"x\" , but see  spatialpermutation  for other options.  #  Images.widthheight     Function .  w, h = widthheight(img)  returns the width and height of an image, regardless of storage order.  See also:  width ,  height .", 
            "title": "Properties accessor-function interface"
        }, 
        {
            "location": "/function_reference/#element-transformation-and-intensity-scaling", 
            "text": "Many images require some type of transformation before you can use or view them. For example, visualization libraries work in terms of 8-bit data, so if you're using a 16-bit scientific camera, your image values will need to be scaled before display.  One can directly rescale the pixel intensities in the image array.  In general, element-wise transformations are handled by  map  or  map! , where the latter is used when you want to provide a pre-allocated output.  You can use an anonymous function of your own design, or, if speed is paramount, the \"anonymous functions\" of the  FastAnonymous  package.  Images also supports \"lazy transformations.\" When loading a very large image, (e.g., loaded by memory-mapping) you may use or view just a small portion of it. In such cases, it would be quite wasteful to force transformation of the entire image, and indeed on might exhaust available memory or need to write a new file on disk.   Images  supports lazy-evaluation scaling through the  MapInfo  abstract type.  The basic syntax is  valout = map(mapi::MapInfo, valin)  Here  val  can refer to a single pixel's data, or to the entire image array. The  mapi  input is a type that determines how the input value is scale and converted to a new type.", 
            "title": "Element transformation and intensity scaling"
        }, 
        {
            "location": "/function_reference/#mapinfo", 
            "text": "Here is how to directly construct the major concrete  MapInfo  types:    MapNone(T) , indicating that the only form of scaling is conversion   to type T.  This can throw an error if a value  x  cannot be   represented as an object of type  T , e.g.,  map(MapNone{U8}, 1.2) .    ClampMin(T, minvalue) ,  ClampMax(T, maxvalue) , and    ClampMinMax(T, minvalue, maxvalue)  create  MapInfo  objects that   clamp pixel values at the specified min, max, and min/max values,   respectively, before converting to type  T . Clamping is equivalent   to  clampedval = min(max(val, minvalue), maxvalue) .    BitShift(T, N)  or  BitShift{T,N}() , for scaling by bit-shift operators.    N  specifies the number of bits to right-shift by.  For example you could   convert a 14-bit image to 8-bits using  BitShift(Uint8, 6) .  In general this   will be faster than using multiplication.    ScaleMinMax(T, min, max, [scalefactor])  clamps the image at the specified   min/max values, subtracts the min value, scales the result by multiplying by    scalefactor , and finally converts the type.  If  scalefactor  is not   specified, it defaults to scaling the range  [min,max]  to  [0,1] .    ScaleAutoMinMax(T)  will cause images to be dynamically scaled to their   specific min/max values, using the same algorithm for  ScaleMinMax . When   displaying a movie, the min/max will be recalculated for each frame, so this   can result in inconsistent contrast scaling.    ScaleSigned(T, scalefactor)  multiplies the image by the scalefactor, then   clamps to the range  [-1,1] . If  T  is a floating-point type, it stays in   this representation.  If  T  is  RGB24  or  RGB{UFixed8} , then it is encoded   as a magenta (positive)/green (negative) image.    There are also convenience functions:  #  Images.imstretch     Function .  imgs = imstretch(img, m, slope)  enhances or reduces (for slope   1 or   1, respectively) the contrast near saturation (0 and 1). This is essentially a symmetric gamma-correction. For a pixel of brightness  p , the new intensity is  1/(1+(m/(p+eps))^slope) .  This assumes the input  img  has intensities between 0 and 1.  #  Images.sc     Function .  imgsc = sc(img)\nimgsc = sc(img, min, max)  Applies default or specified  ScaleMinMax  mapping to the image.  #  Images.MapInfo     Type .  MapInfo{T}  is an abstract type that encompasses objects designed to perform intensity or color transformations on pixels.  For example, before displaying an image in a window, you might need to adjust the contrast settings;  MapInfo  objects provide a means to describe these transformations without calculating them immediately.  This delayed execution can be useful in many contexts.  For example, if you want to display a movie, it would be quite wasteful to have to first transform the entire movie; instead,  MapInfo  objects allow one to specify a transformation to be performed on-the-fly as particular frames are displayed.  You can create your own custom  MapInfo  objects. For example, given a grayscale image, you could color \"saturated\" pixels red using  immutable ColorSaturated{C :AbstractRGB}  : MapInfo{C}\nend\n\nBase.map{C}(::ColorSaturated{C}, val::Union{Number,Gray}) = ifelse(val == 1, C(1,0,0), C(val,val,val))\n\nimgc = map(ColorSaturated{RGB{U8}}(), img)  For pre-defined types see  MapNone ,  BitShift ,  ClampMinMax ,  ScaleMinMax ,  ScaleAutoMinMax , and  ScaleSigned .  #  Images.mapinfo     Function .  mapi = mapinf(T, img)  returns a  MapInfo  object that is deemed appropriate for converting pixels of  img  to be of type  T .  T  can either be a specific type (e.g.,  RGB24 ), or you can specify an abstract type like  Clamp  and it will return one of the  Clamp  family of  MapInfo  objects.  You can define your own rules for  mapinfo .  For example, the  ImageMagick  package defines methods for how pixels values should be converted before saving images to disk.", 
            "title": "MapInfo"
        }, 
        {
            "location": "/function_reference/#color-conversion", 
            "text": "convert  convert(Image{Color}, img)  as described above. Use  convert(Image{Gray}, img)  to calculate a grayscale representation of a color image using the  Rec 601 luma .  map  map(mapi, img)\nmap!(mapi, dest, img)  can be used to specify both the form of the result and the algorithm used.", 
            "title": "Color conversion"
        }, 
        {
            "location": "/function_reference/#image-io", 
            "text": "Image loading and saving is handled by the  FileIO  package.", 
            "title": "Image I/O"
        }, 
        {
            "location": "/function_reference/#image-algorithms", 
            "text": "You can perform arithmetic with  Image s and  Color s. Algorithms also include the following functions:", 
            "title": "Image algorithms"
        }, 
        {
            "location": "/function_reference/#linear-filtering-and-padding", 
            "text": "imfilter\nimfilter!\nimfilter_fft\nimfilter_gaussian\nimfilter_LoG\nimgradients\nmagnitude\nphase\norientation\nmagnitude_phase\nimedge\nthin_edges\nforwarddiffx\nforwarddiffy\nbackdiffx\nbackdiffy\npadarray", 
            "title": "Linear filtering and padding"
        }, 
        {
            "location": "/function_reference/#feature-extraction", 
            "text": "#  Images.blob_LoG     Function .  blob_LoG(img, sigmas) -  Vector{Tuple}  Find \"blobs\" in an N-D image using Lapacian of Gaussians at the specifed sigmas.  Returned are the local maxima's heights, radii, and spatial coordinates.  See Lindeberg T (1998), \"Feature Detection with Automatic Scale Selection\", International Journal of Computer Vision, 30(2), 79\u2013116.  Note that only 2-D images are currently supported due to a limitation of  imfilter_LoG .  #  Images.findlocalmaxima     Function .  findlocalmaxima(img, [region, edges]) -  Vector{Tuple}  Returns the coordinates of elements whose value is larger than all of their immediate neighbors.   region  is a list of dimensions to consider.   edges  is a boolean specifying whether to include the first and last elements of each dimension.  #  Images.findlocalminima     Function .  Like  findlocalmaxima , but returns the coordinates of the smallest elements.", 
            "title": "Feature Extraction"
        }, 
        {
            "location": "/function_reference/#exposure", 
            "text": "#  Images.imhist     Function .  edges, count = imhist(img, nbins)\nedges, count = imhist(img, nbins, minval, maxval)  Generates a histogram for the image over nbins spread between  (minval, maxval] . If  minval  and  maxval  are not given, then the minimum and maximum values present in the image are taken.  edges  is a vector that specifies how the range is divided;  count[i+1]  is the number of values  x  that satisfy  edges[i]  = x   edges[i+1] .  count[1]  is the number satisfying  x   edges[1] , and  count[end]  is the number satisfying  x  = edges[end] . Consequently,  length(count) == length(edges)+1 .", 
            "title": "Exposure"
        }, 
        {
            "location": "/function_reference/#filtering-kernels", 
            "text": "#  Images.gaussian2d     Function .  kern = gaussian2d(sigma, filtersize)  returns a kernel for FIR-based Gaussian filtering.  See also:  imfilter_gaussian .  #  Images.imaverage     Function .  kern = imaverage(filtersize)  constructs a boxcar-filter of the specified size.  #  Images.imdog     Function .  kern = imdog(sigma)  creates a difference-of-gaussians kernel ( sigma s differing by a factor of  sqrt(2) ).  #  Images.imlaplacian     Function .  kern = imlaplacian(filtersize)  returns a kernel for laplacian filtering.  #  Images.imlog     Function .  kern = imlog(sigma)  returns a laplacian-of-gaussian kernel.  #  Images.sobel     Function .  kern1, kern2 = sobel()  returns Sobel filters for dimensions 1 and 2 of your image  #  Images.prewitt     Function .  kern1, kern2 = prewitt()  returns Prewitt filters for dimensions 1 and 2 of your image  #  Images.ando3     Function .  kern1, kern2 = ando3()  returns optimal 3x3 filters for dimensions 1 and 2 of your image, as defined in Ando Shigeru, IEEE Trans. Pat. Anal. Mach. Int., vol. 22 no 3, March 2000.  See also:  ando4 ,  ando5 .  #  Images.ando4     Function .  kern1, kern2 = ando4()  returns optimal 4x4 filters for dimensions 1 and 2 of your image, as defined in Ando Shigeru, IEEE Trans. Pat. Anal. Mach. Int., vol. 22 no 3, March 2000.  See also:  ando4_sep ,  ando3 ,  ando5 .  #  Images.ando5     Function .  kern1, kern2 = ando5()  returns optimal 5x5 filters for dimensions 1 and 2 of your image, as defined in Ando Shigeru, IEEE Trans. Pat. Anal. Mach. Int., vol. 22 no 3, March 2000.  See also:  ando5_sep ,  ando3 ,  ando4 .", 
            "title": "Filtering kernels"
        }, 
        {
            "location": "/function_reference/#nonlinear-filtering-and-transformation", 
            "text": "#  Images.imROF     Function .  imgr = imROF(img, lambda, iterations)  Perform Rudin-Osher-Fatemi (ROF) filtering, more commonly known as Total Variation (TV) denoising or TV regularization.  lambda  is the regularization coefficient for the derivative, and  iterations  is the number of relaxation iterations taken. 2d only.  #  Images.imcorner     Function .  imge = imcorner(img; [method], [border], [blockSize], [k])  Performs corner detection, using either the Harris method or the Shi-Tomasi method.  method  is  'harris'  (default) or  'shi-tomasi' .  'border'  is the border  mode used in computing the gradient (default is  'replicate' ).  'blockSize'  is the size of the box filter kernel (default of  3 ).  k  is the Harris free parameter  (default of  0.04 ). It is only used when method is set to  'harris' .", 
            "title": "Nonlinear filtering and transformation"
        }, 
        {
            "location": "/function_reference/#resizing", 
            "text": "#  Images.restrict     Function .  imgr = restrict(img[, region])  performs two-fold reduction in size along the dimensions listed in  region , or all spatial coordinates if  region  is not specified.  It anti-aliases the image as it goes, so is better than a naive summation over 2x2 blocks.", 
            "title": "Resizing"
        }, 
        {
            "location": "/function_reference/#image-statistics", 
            "text": "#  Images.minfinite     Function .  m = minfinite(A)  calculates the minimum value in  A , ignoring any values that are not finite (Inf or NaN).  #  Images.maxfinite     Function .  m = maxfinite(A)  calculates the maximum value in  A , ignoring any values that are not finite (Inf or NaN).  #  Images.maxabsfinite     Function .  m = maxabsfinite(A)  calculates the maximum absolute value in  A , ignoring any values that are not finite (Inf or NaN).  #  Images.meanfinite     Function .  M = meanfinite(img, region)  calculates the mean value along the dimensions listed in  region , ignoring any non-finite values.  #  Images.ssd     Function .  s = ssd(A, B)  computes the sum-of-squared differences over arrays/images A and B  #  Images.ssdn     Function .  s = ssdn(A, B)  computes the sum-of-squared differences over arrays/images A and B, normalized by array size  #  Images.sad     Function .  s = sad(A, B)  computes the sum-of-absolute differences over arrays/images A and B  #  Images.sadn     Function .  s = sadn(A, B)  computes the sum-of-absolute differences over arrays/images A and B, normalized by array size", 
            "title": "Image statistics"
        }, 
        {
            "location": "/function_reference/#morphological-operations", 
            "text": "#  Images.dilate     Function .  imgd = dilate(img, [region])  perform a max-filter over nearest-neighbors. The default is 8-connectivity in 2d, 27-connectivity in 3d, etc. You can specify the list of dimensions that you want to include in the connectivity, e.g.,  region = [1,2]  would exclude the third dimension from filtering.  #  Images.erode     Function .  imge = erode(img, [region])  perform a min-filter over nearest-neighbors. The default is 8-connectivity in 2d, 27-connectivity in 3d, etc. You can specify the list of dimensions that you want to include in the connectivity, e.g.,  region = [1,2]  would exclude the third dimension from filtering.  #  Images.opening     Function .  imgo = opening(img, [region])  performs the  opening  morphology operation, equivalent to  dilate(erode(img)) .  region  allows you to control the dimensions over which this operation is performed.  #  Images.closing     Function .  imgc = closing(img, [region])  performs the  closing  morphology operation, equivalent to  erode(dilate(img)) .  region  allows you to control the dimensions over which this operation is performed.  #  Images.tophat     Function .  imgth = tophat(img, [region])  performs  top hat  of an image, which is defined as the image minus its morphological opening.  region  allows you to control the dimensions over which this operation is performed.  #  Images.bothat     Function .  imgbh = bothat(img, [region])  performs  bottom hat  of an image, which is defined as its morphological closing minus the original image.  region  allows you to control the dimensions over which this operation is performed.  #  Images.morphogradient     Function .  imgmg = morphogradient(img, [region])  returns morphological gradient of the image, which is the difference between the dilation and the erosion of a given image.  region  allows you to control the dimensions over which this operation is performed.  #  Images.morpholaplace     Function .  imgml = morpholaplace(img, [region])  performs  Morphological Laplacian  of an image, which is defined as the arithmetic difference between the internal and the external gradient.  region  allows you to control the dimensions over which this operation is performed.  #  Images.label_components     Function .  label = label_components(tf, [connectivity])\nlabel = label_components(tf, [region])  Find the connected components in a binary array  tf . There are two forms that  connectivity  can take:    It can be a boolean array of the same dimensionality as  tf , of size 1 or 3 along each dimension. Each entry in the array determines whether a given neighbor is used for connectivity analyses. For example,  connectivity = trues(3,3)  would use 8-connectivity and test all pixels that touch the current one, even the corners.    You can provide a list indicating which dimensions are used to determine connectivity. For example,  region = [1,3]  would not test neighbors along dimension 2 for connectivity. This corresponds to just the nearest neighbors, i.e., 4-connectivity in 2d and 6-connectivity in 3d.    The default is  region = 1:ndims(A) .  The output  label  is an integer array, where 0 is used for background pixels, and each connected region gets a different integer index.  #  Images.component_boxes     Function .  component_boxes(labeled_array)  -  an array of bounding boxes for each label, including the background label 0  #  Images.component_lengths     Function .  component_lengths(labeled_array)  -  an array of areas (2D), volumes (3D), etc. for each label, including the background label 0  #  Images.component_indices     Function .  component_indices(labeled_array)  -  an array of pixels for each label, including the background label 0  #  Images.component_subscripts     Function .  component_subscripts(labeled_array)  -  an array of pixels for each label, including the background label 0  #  Images.component_centroids     Function .  component_centroids(labeled_array)  -  an array of centroids for each label, including the background label 0", 
            "title": "Morphological operations"
        }, 
        {
            "location": "/function_reference/#phantoms", 
            "text": "#  Images.shepp_logan     Function .  phantom = shepp_logan(N,[M]; highContrast=true)  output the NxM Shepp-Logan phantom, which is a standard test image usually used for comparing image reconstruction algorithms in the field of computed tomography (CT) and magnetic resonance imaging (MRI). If the argument M is omitted, the phantom is of size NxN. When setting the keyword argument  `highConstrast  to false, the CT version of the phantom is created. Otherwise, the high contrast MRI version is calculated.", 
            "title": "Phantoms"
        }
    ]
}